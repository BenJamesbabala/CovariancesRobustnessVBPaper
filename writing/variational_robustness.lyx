#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% For LaTeX2e

% Set to togglefalse to use nips formatting.
% Set to toggletrue to use arxiv formatting.
%\usepackage{etoolbox}
%\newtoggle{arxivformat}

\usepackage{times}

\usepackage{url}

\usepackage{subcaption}
\usepackage{caption}

\usepackage{amsthm}
\usepackage{appendix}

% break equations across pages
%\allowdisplaybreaks

% Used to number single equations in an equation array.
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{bbm}


% references
% Since many figures and labels are generated by Knitr, we cannot always use
% Lyx's cross references. 
\newcommand{\fig}[1]{Fig.~(\ref{fig:#1})}

% Custom references for prettyref
\newrefformat{app}{Appendix \ref{#1}}
\newrefformat{eq}{Eq.~(\ref{#1})}
\newrefformat{assu}{Assumption~(\ref{#1})}
\newrefformat{subsec}{Section~(\ref{#1})}
\newrefformat{cor}{Corollary~(\ref{#1})}
\newrefformat{thm}{Theorem~(\ref{#1})}
\newrefformat{def}{Definition~(\ref{#1})}
\newrefformat{prop}{Proposition~(\ref{#1})}
\newrefformat{tab}{Table~(\ref{#1})}

% theorems
%\theoremstyle{plain}
%\newtheorem{theorem}{}[section]
%\newtheorem{proposition}[theorem]{}\newtheorem{lemma}[theorem]{}\newtheorem{remark}[theorem]{}

\title{Covariances, Robustness, and Variational Bayes}

\author{
Ryan Giordano\\
Department of Statistics\\
UC Berkeley
%University of California, Berkeley
%Berkeley, CA 94720 \\
%\texttt{rgiordano@berkeley.edu}
\and
Tamara Broderick \\
Department of EECS\\
MIT
%Cambridge, MA 02139\\
%\texttt{tbroderick@csail.mit.edu}
\and
Michael I. Jordan \\
Departments of EECS and Statistics\\
UC Berkeley
%University of California, Berkeley\\
%Berkeley, CA 94720 \\
%\texttt{jordan@cs.berkeley.edu }
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\end_preamble
\use_default_options false
\begin_modules
knitr
theorems-ams
theorems-ams-extended
theorems-sec
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Math macros
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Basics
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\constant}{Constant}
{Constant}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\tconst}[1][t]{C_{#1}}
{C_{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trans}{\intercal}
{\intercal}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\indep}{\stackrel{indep}{\sim}}
{\stackrel{indep}{\sim}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\iid}{\stackrel{iid}{\sim}}
{\stackrel{iid}{\sim}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\kl}{\mathrm{KL}}
{\mathrm{KL}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\cov}{\mathrm{Cov}}
{\mathrm{Cov}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\var}{\mathrm{Var}}
{\mathrm{Var}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\normal}{\mathcal{N}}
{\mathcal{N}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
{\mathrm{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
{\mathrm{argmax}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Distributions
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\pthetapost}[1][\alpha]{p_{#1}^{x}}
{p_{#1}^{x}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qthetapost}[1][\alpha]{q_{#1}^{x}}
{q_{#1}^{x}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prior}[2][\theta][\alpha]{p\left(#1\vert#2\right)}
{p\left(#1\vert#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\ptthetapost}{\pthetapost[\alpha,t]}
{\pthetapost[\alpha,t]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qtthetapost}{\qthetapost[\alpha,t]}
{\qthetapost[\alpha,t]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\etaopt}{\eta^{*}}
{\eta^{*}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\etatopt}{\eta^{*}\left(t\right)}
{\eta^{*}\left(t\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qthetapostarg}{\qthetapost\left(\theta;\eta^{*}\right)}
{\qthetapost\left(\theta;\eta^{*}\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qtthetapostarg}{\qthetapost\left(\theta;\eta^{*}\left(t\right)\right)}
{\qthetapost\left(\theta;\eta^{*}\left(t\right)\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\gtheta}[1][\theta]{g\left(#1\right)}
{g\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbeq}{\mbe_{\qthetapost}}
{\mathbb{E}_{\qthetapost}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbep}{\mbe_{\pthetapost}}
{\mathbb{E}_{\pthetapost}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\epgtheta}{\mbe_{\pthetapost}\left[\gtheta\right]}
{\mbe_{\pthetapost}\left[\gtheta\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\eqgtheta}{\mbe_{\qthetapost}\left[\gtheta\right]}
{\mbe_{\qthetapost}\left[\gtheta\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lrvbcov}{\mathrm{Cov}_{\qthetapost}^{LR}}
{\mathrm{Cov}_{\qthetapost}^{LR}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\klhess}{\mathbf{H}_{\eta\eta}}
{\mathbf{H}_{\eta\eta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\efhess}{\mathbf{f}_{t\eta}}
{\mathbf{f}_{t\eta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\eggrad}{\mathbf{g}_{\eta}}
{\mathbf{g}_{\eta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\covmat}{\boldsymbol{\Sigma}}
{\boldsymbol{\Sigma}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\infomat}{\boldsymbol{\Lambda}}
{\boldsymbol{\Lambda}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Sensitivity
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\psens}{\mathbf{S}_{\alpha}}
{\mathbf{S}_{\alpha}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qsens}{\mathbf{S}_{\alpha}^{q}}
{\mathbf{S}_{\alpha}^{q}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\psenshat}{\hat{\mathbf{S}}_{\alpha}}
{\hat{\mathbf{S}}_{\alpha}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\contampriornoarg}{u}
{u}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\contamprior}[1][\theta]{\contampriornoarg\left(#1\right)}
{\contampriornoarg\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\psensfunc}{S_{\contampriornoarg}}
{S_{\contampriornoarg}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qsensfunc}{S_{\contampriornoarg}^{q}}
{S_{\contampriornoarg}^{q}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qsenssup}{S_{sup}^{q}}
{S_{sup}^{q}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\psenssup}{S_{sup}}
{S_{sup}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\contamnorm}{C_{u}}
{C_{u}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\origprior}[1][\theta]{p_{0}\left(#1\right)}
{p_{0}\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\influence}[2][\phantom{}][\phantom{}]{I_{#2}^{#1}\left(\theta\right)}
{I_{#2}^{#1}\left(\theta\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\influenceplus}{\influence[+]}
{\influence[+]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qinfluence}[1][\phantom{}]{\influence[#1][q]}
{\influence[#1][q]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\covdens}[2][\theta][t]{\rho\left(#1,#2\right)}
{\rho\left(#1,#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\covdensnorm}[2][\theta][t]{p\left(#1\vert#2\right)}
{p\left(#1\vert#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\linearoperator}[2][\theta][\,]{L^{#2}\left(#1\right)}
{L^{#2}\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Laplace and EM
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\laptheta}{\hat{\theta}_{Lap}}
{\hat{\theta}_{Lap}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\laphess}{\mathbf{H}_{Lap}}
{\mathbf{H}_{Lap}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lap}{p_{Lap,\alpha}^{x}}
{p_{Lap,\alpha}^{x}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lapgtheta}{g_{\laptheta}}
{g_{\laptheta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
initialization, echo=FALSE, message=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

# Load libraries and set global knitr options.
\end_layout

\begin_layout Plain Layout

source("R_graphs/Initialize.R", echo=FALSE)
\end_layout

\begin_layout Plain Layout

source("R_graphs/ImageSizeCommands.R", echo=FALSE)
\end_layout

\begin_layout Plain Layout

source("R_graphs/CommonGraphs.R", echo=FALSE) 
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Variational Bayes (VB) is an approximate Bayesian posterior inference technique
 that is increasingly popular due to its fast runtimes on large-scale datasets.
 However, even when VB provides accurate posterior means for certain parameters,
 it often mis-estimates variances and covariances.
 Furthermore, prior robustness measures have remained undeveloped for VB.
 By deriving a simple formula for the effect of infinitesimal model perturbation
s on VB posterior means, we provide both improved covariance estimates and
 local robustness measures for VB, thus greatly expanding the practical
 usefulness of VB posterior approximations.
 The estimates for VB posterior covariances rely on a result from the classical
 Bayesian robustness literature relating derivatives of posterior expectations
 to posterior covariances.
 Our key assumption is that the VB approximation provides good estimates
 of a select subset of posterior means—an assumption that has been shown
 to hold in many practical settings.
 In our experiments, we demonstrate that our methods are simple, general,
 and fast, providing accurate posterior uncertainty estimates and robustness
 measures with runtimes that can be an order of magnitude  smaller than
 MCMC.
\end_layout

\begin_layout Abstract
Key phrases: Variational Bayes; Bayesian robustness; Mean field approximation;
 Linear response theory; Automatic differentiation
\end_layout

\begin_layout Section
Introduction
\begin_inset CommandInset label
LatexCommand label
name "sec:intro"

\end_inset

 
\end_layout

\begin_layout Standard
Most Bayesian posteriors cannot be calculated analytically, so in practice
 we turn to approximations.
 Variational Bayes (VB) casts posterior approximation as an optimization
 problem in which the objective to be minimized is the divergence, among
 a tractable sub-class of posteriors, from the exact posterior.
 For example, one widely-used and relatively simple flavor of VB is 
\begin_inset Quotes eld
\end_inset

mean field variational Bayes
\begin_inset Quotes erd
\end_inset

 (MFVB), which employs Kullback-Liebler (KL) divergence and a factorizing
 exponential family approximation for the tractable sub-class of posteriors
 
\begin_inset CommandInset citation
LatexCommand citep
key "wainwright2008graphical"

\end_inset

.
 MFVB has been increasingly popular as an alternative to Markov Chain Monte
 Carlo (MCMC) in part due to its fast runtimes on large-scale data sets.
 Although MFVB does not come with any general accuracy guarantees (except
 asymptotic ones in special cases, as in 
\begin_inset CommandInset citation
LatexCommand citet
key "westling:2015:vbconsistency,wang:2017:vbconsistency"

\end_inset

), MFVB produces posterior mean estimates of certain parameters that are
 accurate enough to be useful in a number of real-world applications 
\begin_inset CommandInset citation
LatexCommand citep
key "blei:2016:variational"

\end_inset

.
 In the current work we will focus on MFVB for motivation and in our examples
 and experiments, though many of the ideas here could be applied to more
 general approximations or divergences.
 Despite its computational advantages, MFVB typically underestimates marginal
 variances 
\begin_inset CommandInset citation
LatexCommand citep
key "mackay:2003:information,wang:2005:inadequacy,turner:2011:two"

\end_inset

, and, to our knowledge, techniques for assessing Bayesian robustness have
 not yet been developed for VB.
  It is these inferential issues that arethe focus of the current paper.
\end_layout

\begin_layout Standard
In contrast to the optimization approach of VB, MCMC constructs a Markov
 chain with the exact posterior as its stationary distribution.
 At first glance, it may seem that MCMC is made for integration and MFVB
 is made for differentiation.
 Using MCMC draws, integrals with respect to the posterior can be readily
 approximated by constructing the corresponding sample moment from the MCMC
 draws.
 Similarly, as a parametric optimization problem, MFVB lends itself naturally
 to sensitivity analysis, since its optimum can be differentiated analytically.
 However, a key result of the Bayesian local robustness literature is that
 derivatives and covariances are two sides of the same coin, since, under
 mild regularity conditions, derivatives of posterior quantities can re-cast
 as posterior covariances by exchanging the order of integration and differentia
tion (
\begin_inset CommandInset citation
LatexCommand citet
key "gustafson:1996:localposterior,basu:1996:local,efron:2015:frequentist"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:cov_and_sens"

\end_inset

 below).
 
\end_layout

\begin_layout Standard
Thus, in order to calculate local prior sensitivity, the Bayesian robustness
 literature re-casts prior sensitivities as posterior covariances that can
 be easily calculated with MCMC.
 In order to provide covariance estimates for MFVB, we turn this idea on
 its head and use the sensitivity of MFVB posterior expectations to estimate
 their covariances.
 Additionally, we derive straightforward general VB (not only MFVB) versions
 of a number of prior sensitivity measures from the Bayesian robustness
 literature, including parametric sensitivity and sensitivity to additive
 functional perturbations.
 Our key assumption is that the posterior means of interest are well-estimated
 by VB for all the perturbations of interest.
 In our experiments, we compare MFVB models to both MCMC and maximum a posterior
i (MAP) posterior approximations.
 We find that the MFVB means and variances, unlike the MAP estimates, match
 the MCMC approximations closely while still running over an order of magnitude
 faster than MCMC.
\end_layout

\begin_layout Standard
We will begin in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:theory"

\end_inset

 by discussing the general relationship between Bayesian sensitivity and
 posterior covariance, and then defining local robustness and sensitivity.
 Next, we will introduce VB and derive the linear system for the VB local
 sensitivity estimates.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:sensitivity_in_action"

\end_inset

, we show how to use the VB local sensitivity results to estimate covariances
 and calculate a range of canonical Bayesian prior sensitivity measures.
 Finally, in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

, we describe our experiments on real industry data that illustrate the
 speed and effectiveness of our methods.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Conclusion"

\end_inset

 concludes.
\end_layout

\begin_layout Section
Bayesian (and variational Bayesian) covariances and sensitivity
\begin_inset CommandInset label
LatexCommand label
name "sec:theory"

\end_inset


\end_layout

\begin_layout Standard
An MCMC posterior estimate is an empirical distribution formed with posterior
 draws, and a VB posterior estimate takes the form of a parameterized distributi
on with optimized parameters.
 MCMC draws lend themselves naturally to the approximate calculation of
 posterior moments, such as those required for covariances.
 In contrast, VB approximations lend themselves naturally to sensitivity
 analysis, since we can analytically differentiate the optima with respect
 to perturbations.
 However, the contrast between derivatives and moments is not so stark since,
 under mild regularity conditions that allow the exchange of integration
 and differentiation, there is a direct correspondence between sensitivity
 and covariance.
 Thus, as has long been known in the Bayesian robustness literature, we
 can use the sample covariances of MCMC to calculate posterior sensitivities.
 With VB approximations, for which sensitivity is more natural, we can apply
 this relationship in reverse: we can use the VB sensitivity to calculate
 covariances.
\end_layout

\begin_layout Subsection
Covariances and sensitivity
\begin_inset CommandInset label
LatexCommand label
name "subsec:cov_and_sens"

\end_inset


\end_layout

\begin_layout Standard
We will first state a general result relating sensitivity and covariance
 and apply it to our specific cases of interest as they arise throughout
 the paper.
 Denote an unknown model parameter by the vector 
\begin_inset Formula $\theta\in\mathbb{R}^{K}$
\end_inset

, and assume a dominating measure for 
\begin_inset Formula $\theta$
\end_inset

 given by 
\begin_inset Formula $\lambda$
\end_inset

.
 Define 
\begin_inset Formula $\covdens$
\end_inset

 to be any 
\begin_inset Formula $\lambda$
\end_inset

-measurable function on 
\begin_inset Formula $\theta$
\end_inset

 that depends on an index 
\begin_inset Formula $t\in\mathbb{R}^{D}$
\end_inset

, and assume that 
\begin_inset Formula $0<\int\exp\left(\rho\left(\theta,t\right)\right)\lambda\left(d\theta\right)<\infty$
\end_inset

.
 For our purposes, one may think of 
\begin_inset Formula $\exp\left(\covdens\right)$
\end_inset

 as the product of a parameterized likelihood and a prior with dependencies
 other than 
\begin_inset Formula $t$
\end_inset

 left implicit.
 Later on, we will choose 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $\rho\left(\theta,t\right)$
\end_inset

 to represent various prior and model perturbations, but for now we will
 keep the discussion abstract.
 After normalization we get a density, 
\begin_inset Formula $\covdensnorm$
\end_inset

, in 
\begin_inset Formula $\theta$
\end_inset

 with respect to 
\begin_inset Formula $\lambda$
\end_inset

 : 
\begin_inset Formula 
\begin{align*}
\covdensnorm & :=\frac{\exp\left(\covdens\right)}{\int\exp\left(\covdens[\theta']\right)\lambda\left(d\theta'\right)}.
\end{align*}

\end_inset

 Suppose we are interested in differentiating the expectation
\begin_inset Formula 
\begin{align*}
\mbe_{\covdensnorm}\left[g\left(\theta\right)\right] & :=\int\covdensnorm\gtheta\lambda\left(d\theta\right)
\end{align*}

\end_inset

with respect to 
\begin_inset Formula $t$
\end_inset

 at 
\begin_inset Formula $t=0$
\end_inset

.
 This will measure the local sensitivity of 
\begin_inset Formula $\mbe_{\covdensnorm}\left[g\left(\theta\right)\right]$
\end_inset

 to the index 
\begin_inset Formula $t$
\end_inset

 at 
\begin_inset Formula $t=0$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:sens_cov"

\end_inset


\end_layout

\begin_layout Theorem
When 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:bayes_ok"

\end_inset

 hold for all 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero, then
\begin_inset Formula 
\begin{align}
\left.\frac{d\mbe_{\covdensnorm}\left[g\left(\theta\right)\right]}{dt^{\trans}}\right|_{t=0} & =\cov_{\covdensnorm[][0]}\left(g\left(\theta\right),\left.\frac{\partial\covdens}{\partial t}\right|_{t=0}\right).\label{eq:covariance_sensitivity_general}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
See 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:sens_and_cov"

\end_inset

 for a proof and technical conditions.
 By using MCMC draws to calculate the covariance on the right-hand side
 of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:covariance_sensitivity_general"

\end_inset

, one can form an estimate of 
\begin_inset Formula $d\mbe_{\covdensnorm}\left[g\left(\theta\right)\right]/dt^{\trans}$
\end_inset

 at 
\begin_inset Formula $t=0$
\end_inset

.
 One might also approach the problem of calculating 
\begin_inset Formula $d\mbe_{\covdensnorm}\left[g\left(\theta\right)\right]/dt^{\trans}$
\end_inset

 using importance sampling 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 9"
key "owen:2013:mcmcbook"

\end_inset

 as follows.
 First, an importance sampling estimate of the dependence of 
\begin_inset Formula $\mbe_{\covdensnorm}\left[\gtheta\right]$
\end_inset

 on 
\begin_inset Formula $t$
\end_inset

 can be constructed with weights that depend on 
\begin_inset Formula $t$
\end_inset

.
 Then, by differentiating the weights with respect to 
\begin_inset Formula $t$
\end_inset

, this would provide a sample-based estimate of 
\begin_inset Formula $d\mbe_{\covdensnorm}\left[g\left(\theta\right)\right]/dt^{\trans}$
\end_inset

.
 We show in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:mcmc_importance_sampling"

\end_inset

 that this is approach is equivalent to using MCMC samples to estimate the
 covariance in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:local_sensitivity"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 will immediately allow us to calculate local sensitivity from MCMC draws
 using sample covariances.
 After a little more work, in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:lrvb_cov"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 will allow us to use the sensitivity of VB means to calculate their covariances.
\end_layout

\begin_layout Subsection
Local sensitivity and robustness
\begin_inset CommandInset label
LatexCommand label
name "subsec:local_sensitivity"

\end_inset


\end_layout

\begin_layout Standard
As in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:cov_and_sens"

\end_inset

, denote an unknown model parameter by the vector 
\begin_inset Formula $\theta\in\mathbb{R}^{K}$
\end_inset

, and assume a dominating measure on 
\begin_inset Formula $\theta$
\end_inset

 given by 
\begin_inset Formula $\lambda$
\end_inset

.
 Denote the prior parameters by 
\begin_inset Formula $\alpha$
\end_inset

, where 
\begin_inset Formula $\alpha\in\mathbb{R}^{M}$
\end_inset

.
 Finally, denote our data by 
\begin_inset Formula $x$
\end_inset

.
 Suppose we have tentatively chosen a likelihood, 
\begin_inset Formula $p\left(x\vert\theta\right)$
\end_inset

, and a prior, 
\begin_inset Formula $\prior$
\end_inset

.
 We then let 
\begin_inset Formula $\pthetapost$
\end_inset

 denote the posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

, as given by Bayes' Theorem:
\begin_inset Formula 
\begin{align*}
\pthetapost\left(\theta\right) & :=p\left(\theta\vert x,\alpha\right)=\frac{p\left(x\vert\theta\right)\prior}{\int p\left(x\vert\theta'\right)\prior[\theta']\lambda\left(d\theta'\right)}.
\end{align*}

\end_inset

We will assume that we are interested in a posterior expectation of some
 function 
\begin_inset Formula $\gtheta$
\end_inset

 (e.g., a parameter mean, a posterior predictive value, or squared loss):
 
\begin_inset Formula $\epgtheta$
\end_inset

.
 In the current work, we will quantify the uncertainty of 
\begin_inset Formula $\epgtheta$
\end_inset

 by the posterior variance, 
\begin_inset Formula $\var_{\pthetapost}\left(\gtheta\right)$
\end_inset

.
 Other measures of central tendency (e.g., posterior medians) or uncertainty
 (e.g., posterior quantiles) may also be good choices, but are beyond the
 scope of the current work.
\end_layout

\begin_layout Standard
Note the dependence of 
\begin_inset Formula $\epgtheta$
\end_inset

 on both the likelihood and prior through Bayes' theorem.
 The choice of a prior and choice of a likelihood is made by the modeler
 and is almost invariably a simplified representation of the real world.
 The choice is therefore to some extent subjective, and so one hopes that
 the salient aspects of the posterior would not vary under reasonable variation
 in the choice of prior and the choice of likelihood.
 Consider the prior, for example: the process of prior elicitation may be
 prohibitively time-consuming; two practitioners may have irreconcilable
 subjective prior beliefs; or the model may be so complex and high-dimensional
 that humans cannot reasonably express their prior beliefs as formal distributio
ns.
 All of these circumstances might give rise to a range of reasonable prior
 choices.
 A posterior quantity is 
\begin_inset Quotes eld
\end_inset

robust
\begin_inset Quotes erd
\end_inset

 to the prior to the extent that it does not change much when calculated
 under these different prior choices.
 Although robustness to the likelihood is no less important, in this paper
 we will focus on prior robustness, in part for continuity with existing
 literature.
\end_layout

\begin_layout Standard
Quantifying the sensitivity of the posterior to variation in the likelihood
 and prior is one of the central concerns of the field of robust Bayes 
\begin_inset CommandInset citation
LatexCommand citep
key "berger:2012:robust"

\end_inset

.
 (We will not discuss the other central concern, which is the selection
 of priors and likelihoods that lead to robust estimators.) Suppose that
 we have determined that the prior parameter 
\begin_inset Formula $\alpha$
\end_inset

 belongs to some set 
\begin_inset Formula $\mathcal{A}$
\end_inset

, perhaps after expert prior elicitation.
 Ideally, we would calculate the extrema of 
\begin_inset Formula $\epgtheta$
\end_inset

 as 
\begin_inset Formula $\alpha$
\end_inset

 ranges over all of 
\begin_inset Formula $\mathcal{A}$
\end_inset

.
 This is called 
\shape italic
global robustness
\shape default
 and is intractable or difficult except in special cases 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 15"
key "moreno:2012:globalrobustness,huber:2011:robust"

\end_inset

.
 An alternative is to examine how much 
\begin_inset Formula $\epgtheta$
\end_inset

 changes locally in response to small perturbations in the value of 
\begin_inset Formula $\alpha$
\end_inset

.
 To this end, we define the 
\shape italic
local sensitivity
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "gustafson:2012:localrobustnessbook"

\end_inset

:
\end_layout

\begin_layout Definition
The local sensitivity of 
\begin_inset Formula $\epgtheta$
\end_inset

 to prior parameter 
\begin_inset Formula $\alpha$
\end_inset

 is given by
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{eqnarray}
\psens & := & \left.\frac{d\epgtheta}{d\alpha}\right|_{\alpha}.\label{eq:local_robustness}
\end{eqnarray}

\end_inset


\begin_inset Formula $\psens$
\end_inset

, the local sensitivity, can be considered a measure of
\emph on
 local robustness
\emph default
 
\begin_inset CommandInset citation
LatexCommand citep
key "gustafson:2012:localrobustnessbook"

\end_inset

.
 Throughout the paper we will distinguish between sensitivity, which comprises
 objectively defined quantities such as 
\begin_inset Formula $\psens$
\end_inset

, and robustness, which we treat as a more subjective decision that may
 be informed by the sensitivity as well as other considerations.
 For example, even if one knows 
\begin_inset Formula $\psens$
\end_inset

 precisely, how much posterior change is too much change and how much prior
 variation is reasonable remain decisions to be made by the modeler.
 For a more in-depth discussion of how we use the terms sensitivity and
 robustness, see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:sens_and_robustness"

\end_inset

.
\end_layout

\begin_layout Definition
 Local sensitivity can be thought of as quantifying sensitivity to priors
 within a small region where the posterior dependence on the prior is approximat
ely linear.
 It provides an approximation to global robustness in the sense that, to
 first order, for 
\begin_inset Formula $\Delta\alpha\in\mathcal{A}-\alpha$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
\left.\epgtheta\right|_{\alpha+\Delta\alpha} & \approx\epgtheta+\psens^{\trans}\Delta\alpha.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
An immediate corollary of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 allows us to calculate 
\begin_inset Formula $\psens$
\end_inset

 as a  covariance.
\end_layout

\begin_layout Corollary
\begin_inset CommandInset label
LatexCommand label
name "cor:sens_cov_prior"

\end_inset


\end_layout

\begin_layout Corollary
When the conditions of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 hold for 
\begin_inset Formula $t=\alpha$
\end_inset

 and 
\begin_inset Formula $\covdens[][\alpha]=\log p\left(x\vert\theta\right)+\log\prior$
\end_inset

 then
\begin_inset Formula 
\begin{align}
\psens & =\cov_{\pthetapost}\left(\gtheta,\frac{\partial\log\prior}{\partial\alpha}\right).\label{eq:covariance_sensitivity}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
See also 
\begin_inset CommandInset citation
LatexCommand citet
key "basu:1996:local"

\end_inset

, in which 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:sens_cov_prior"

\end_inset

 is stated in the proof of Theorem 1, as well as 
\begin_inset CommandInset citation
LatexCommand citet
key "perez:2006:mcmc"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "efron:2015:frequentist"

\end_inset

.
 Given MCMC draws from a chain we assume to have reached equilibrium with
 stationary distribution 
\begin_inset Formula $\pthetapost$
\end_inset

, one can calculate an estimate of 
\begin_inset Formula $\psens$
\end_inset

 using the sample covariance version of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:covariance_sensitivity_general"

\end_inset

:
\begin_inset Formula 
\begin{align}
\psenshat & :=\frac{1}{N_{s}}\sum_{n=1}^{N_{s}}g\left(\theta_{n}\right)\frac{\partial\log\prior[\theta_{n}]}{\partial\alpha^{\trans}}-\left(\frac{1}{N_{s}}\sum_{n=1}^{N_{s}}g\left(\theta_{n}\right)\right)\left(\frac{1}{N_{s}}\sum_{n=1}^{N_{s}}\frac{\partial\log\prior[\theta_{n}]}{\partial\alpha^{\trans}}\right)\label{eq:mcmc_sample_cov}\\
\textrm{for }\theta_{n} & \iid\pthetapost\left(\theta\right),\textrm{ for }n=1,...,N_{s}.\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Variational Bayes
\begin_inset CommandInset label
LatexCommand label
name "subsec:variational_Bayes"

\end_inset


\end_layout

\begin_layout Standard
We now briefly review VB and state our key assumptions about its accuracy.
 We wish to find an approximate distribution, in some class 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 of tractable distributions, selected to minimize the Kullback-Liebler divergenc
e (KL divergence) between 
\begin_inset Formula $q\in\mathcal{Q}$
\end_inset

 and the exact posterior 
\begin_inset Formula $\pthetapost$
\end_inset

.
 We assume that distributions in 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 are parameterized by a finite-dimensional parameter 
\begin_inset Formula $\eta$
\end_inset

 in some feasible set 
\begin_inset Formula $\Omega_{\eta}\subseteq\mathbb{R}^{K_{\eta}}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray}
\mathcal{Q} & := & \left\{ q:q=q\left(\theta;\eta\right)\textrm{ for }\eta\in\Omega_{\eta}\right\} .\label{eq:q_approximating_family}
\end{eqnarray}

\end_inset

Given 
\begin_inset Formula $\mathcal{Q}$
\end_inset

, we define the optimal 
\begin_inset Formula $q\in\mathcal{Q}$
\end_inset

, which we call 
\begin_inset Formula $\qthetapost$
\end_inset

, as that distribution that minimizes the KL divergence 
\begin_inset Formula $KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)$
\end_inset

 from 
\begin_inset Formula $\pthetapost$
\end_inset

.
 We denote the corresponding optimal variational parameters as 
\begin_inset Formula $\etaopt$
\end_inset

.
\end_layout

\begin_layout Definition
The variational approximation 
\begin_inset Formula $\qthetapost\left(\theta\right)$
\end_inset

 to 
\begin_inset Formula $\pthetapost\left(\theta\right)$
\end_inset

 is defined by
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{eqnarray}
\qthetapost\left(\theta\right)=q\left(\theta;\etaopt\right) & := & \textrm{argmin}_{q\in\mathcal{Q}}\left\{ KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)\right\} \label{eq:kl_divergence}
\end{eqnarray}

\end_inset

where
\begin_inset Formula 
\begin{align*}
KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right) & =\mbe_{q\left(\theta;\eta\right)}\left[\log q\left(\theta;\eta\right)-\log p\left(x\vert\theta\right)-\log p\left(\theta\vert\alpha\right)\right]+\log p\left(x\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
In the KL divergence, the (generally intractable) normalizing term 
\begin_inset Formula $\log p\left(x\right)$
\end_inset

 does not depend on 
\begin_inset Formula $q\left(\theta\right)$
\end_inset

 and so can be neglected in the optimization.
 In order for the KL divergence to be well defined, we assume that both
 
\begin_inset Formula $p\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $q\left(\theta\right)$
\end_inset

 are given with respect to the same base measure, 
\begin_inset Formula $\lambda$
\end_inset

, and that the support of 
\begin_inset Formula $q\left(\theta\right)$
\end_inset

 is contained in the support of 
\begin_inset Formula $p\left(\theta\right)$
\end_inset

.
 We additionally assume that 
\begin_inset Formula $KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)$
\end_inset

 is twice differentiable as a function of 
\begin_inset Formula $\eta$
\end_inset

, that the optimal 
\begin_inset Formula $\etaopt$
\end_inset

 is interior to 
\begin_inset Formula $\Omega_{\eta}$
\end_inset

, and that 
\begin_inset Formula $\etaopt$
\end_inset

 varies smoothly in 
\begin_inset Formula $\alpha$
\end_inset

 (rigorous statements of these assumptions can be found in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:lrvb"

\end_inset

).
 
\end_layout

\begin_layout Standard
A common choice for the approximating family 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:q_approximating_family"

\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

mean field family
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "wainwright2008graphical,blei:2016:variational"

\end_inset

,
\begin_inset Formula 
\begin{align}
\mathcal{Q}_{mf} & :=\left\{ q\left(\theta\right):q\left(\theta\right)=\prod_{k}q\left(\theta_{k};\eta_{k}\right)\right\} ,\label{eq:q_mean_field_family}
\end{align}

\end_inset

where 
\begin_inset Formula $k$
\end_inset

 indexes a partition of the full vector 
\begin_inset Formula $\theta$
\end_inset

 and of the parameter vector 
\begin_inset Formula $\eta$
\end_inset

.
 That is, 
\begin_inset Formula $\mathcal{Q}_{mf}$
\end_inset

 approximates the posterior 
\begin_inset Formula $\pthetapost$
\end_inset

 as a distribution that factorizes across sub-components of 
\begin_inset Formula $\theta$
\end_inset

.
 Note that, in general, each function 
\begin_inset Formula $q\left(\theta_{k};\eta_{k}\right)$
\end_inset

 in the product is different.
 For notational convenience we write 
\begin_inset Formula $q\left(\theta_{k};\eta_{k}\right)$
\end_inset

 instead of 
\begin_inset Formula $q_{k}\left(\theta_{k};\eta_{k}\right)$
\end_inset

 when the arguments make it clear which function we are referring to, much
 as the same symbol 
\begin_inset Formula $p$
\end_inset

 is used to refer to many different probability distributions without additional
 indexing.
 One may additionally assume that the components 
\begin_inset Formula $q\left(\theta_{k};\eta_{k}\right)$
\end_inset

 are in a convenient exponential family.
 We will refer to the use of VB with this choice of the factorization and
 the exponential family assumption as 
\begin_inset Quotes eld
\end_inset

MFVB,
\begin_inset Quotes erd
\end_inset

 for 
\begin_inset Quotes eld
\end_inset

mean field variational Bayes.
\begin_inset Quotes erd
\end_inset

 For example, in the case of MFVB, 
\begin_inset Formula $\Omega_{\eta}$
\end_inset

 could be a stacked vector of the natural parameters of the exponential
 families, or the moment parameterization, or perhaps a transformation of
 these parameters into an unconstrained space (e.g., the entries of log-Cholesky
 decomposition of a positive definite information matrix).
 For concrete examples, see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

.
 Although all of our experiments will use MFVB, our results extend to other
 choices of 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 that satisfy the necessary assumptions.
 
\end_layout

\begin_layout Standard
Recall that we are interested in 
\begin_inset Formula $\epgtheta$
\end_inset

.
 Our core assumption will be that, for a range of 
\begin_inset Formula $\alpha$
\end_inset

, the variational distribution provides a good approximation to 
\begin_inset Formula $\epgtheta$
\end_inset

 and its directional derivatives.
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:vb_accurate"

\end_inset

For a given function of interest, 
\begin_inset Formula $\gtheta$
\end_inset

, a given open set of plausible prior parameters 
\begin_inset Formula $\mathcal{A}$
\end_inset

, and for all 
\begin_inset Formula $\alpha,\alpha'\in\mathcal{A}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{\qthetapost}\left[\gtheta\right] & \approx & \mbe_{\pthetapost}\left[\gtheta\right]\textrm{ and }\\
\frac{d\mbe_{\qthetapost[\alpha]}\left[\gtheta\right]}{d\alpha^{\trans}}\left(\alpha'-\alpha\right) & \approx & \frac{d\mbe_{\pthetapost}\left[\gtheta\right]}{d\alpha^{\trans}}\left(\alpha'-\alpha\right).
\end{eqnarray*}

\end_inset


\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
We will not attempt to be precise about what we mean by the 
\begin_inset Quotes eld
\end_inset

approximately equal
\begin_inset Quotes erd
\end_inset

 sign, since we are not aware of any tools for evaluating quantitatively
 whether 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds other than running both VB and MCMC (or some other slow but accurate
 posterior approximation) and comparing the results.
 However, VB has been useful in practice to the extent that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds true for at least some parameters of interest.
 We will evaluate 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 in each of our experiments below by comparing the VB and MCMC posterior
 approximate means.
 
\end_layout

\begin_layout Standard
Since 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds only for a particular choice of 
\begin_inset Formula $\gtheta$
\end_inset

, it is weaker than the assumption that 
\begin_inset Formula $\qthetapost$
\end_inset

 is close to 
\begin_inset Formula $\pthetapost$
\end_inset

 in KL divergence, or even that all the posterior means are accurately estimated.
 For example, as discussed in Appendix B of 
\begin_inset CommandInset citation
LatexCommand citet
key "giordano:2015:lrvb"

\end_inset

 and in Section 10.1.2 of 
\begin_inset CommandInset citation
LatexCommand citet
key "bishop:2006:pattern"

\end_inset

, a mean field approximation to a multivariate normal posterior produces
 inaccurate covariances and may have an arbitrarily bad KL divergence from
 
\begin_inset Formula $\pthetapost$
\end_inset

, but 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds exactly for the location parameters.
 We discuss the multivariate normal example further in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:lrvb_cov"

\end_inset

 below.
\end_layout

\begin_layout Subsection
Variational Bayes sensitivity
\begin_inset CommandInset label
LatexCommand label
name "subsec:vb_sensitivity"

\end_inset

 
\end_layout

\begin_layout Standard
Just as MCMC approximations lend themselves to moment calculations, the
 variational form of VB approximations lends itself to sensitivity calculations.
 In this section, as with 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

, we derive the sensitivity of VB posterior means to generic perturbations.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:sensitivity_in_action"

\end_inset

 we will choose particular perturbations to calculate VB prior sensitivity
 and, through 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

, posterior covariances.
\end_layout

\begin_layout Standard
Consider a generic class of log-perturbations defined by some function,
 
\begin_inset Formula $f\left(\theta,t\right)\in\mathbb{R}$
\end_inset

, parameterized by a vector 
\begin_inset Formula $t\in\mathbb{R}^{n}$
\end_inset

, with 
\begin_inset Formula $f\left(\theta,0\right)=0$
\end_inset

.
 Given a choice of 
\begin_inset Formula $f\left(\theta,t\right)$
\end_inset

, let us consider the posterior defined by
\begin_inset Formula 
\begin{eqnarray}
\ptthetapost\left(\theta\right) & = & \frac{p\left(\theta\vert x\right)p\left(\theta\vert\alpha\right)\exp\left(f\left(\theta,t\right)\right)}{\int p\left(\theta'\vert x\right)p\left(\theta'\vert\alpha\right)\exp\left(f\left(\theta',t\right)\right)d\theta'}.\label{eq:p_tilting}
\end{eqnarray}

\end_inset

In the notation of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

, we are taking 
\begin_inset Formula $\rho\left(\theta,t\right)=\log p\left(\theta\vert x\right)+\log\prior+f\left(\theta,t\right)$
\end_inset

.
 Assuming regularity conditions, given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:lrvb"

\end_inset

, we can define a variational approximation to this perturbed model:
\begin_inset Formula 
\begin{eqnarray}
\qtthetapost\left(\theta\right) & := & \textrm{argmin}_{q\in\mathcal{Q}}\left\{ KL\left(q\left(\theta;\eta\right)||\ptthetapost\left(\theta\right)\right)\right\} .\label{eq:perturbed_vb_approximation}
\end{eqnarray}

\end_inset

This variational approximation will be a function of 
\begin_inset Formula $t$
\end_inset

 through the optimal parameters 
\begin_inset Formula $\etatopt$
\end_inset

, i.e., 
\begin_inset Formula $\qtthetapost\left(\theta\right)=\qtthetapostarg$
\end_inset

.
 For notational convenience, we will define the following quantities.
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:vb_derivatives"

\end_inset

Define the following derivatives of variational expectations evaluated at
 the optimal parameters:
\end_layout

\begin_layout Definition
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\klhess:=\left.\frac{\partial^{2}KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)}{\partial\eta\partial\eta^{\trans}}\right|_{\eta=\etaopt}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\efhess:=\left.\frac{\partial\mbe_{q\left(\theta;\eta\right)}\left[f\left(\theta,t\right)\right]}{\partial t\partial\eta^{\trans}}\right|_{\eta=\etaopt,t=0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\eggrad:=\left.\frac{\partial\mbe_{q\left(\theta;\eta\right)}\left[g\left(\theta\right)\right]}{\partial\eta^{\trans}}\right|_{\eta=\etaopt}.$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $\gtheta$
\end_inset

, 
\begin_inset Formula $t$
\end_inset

, and 
\begin_inset Formula $\eta$
\end_inset

 are all vectors, the quantities 
\begin_inset Formula $\klhess$
\end_inset

, 
\begin_inset Formula $\efhess$
\end_inset

, and 
\begin_inset Formula $\eggrad$
\end_inset

 are matrices.
 With these definitions in hand we can now state:
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:lrvb_formula"

\end_inset


\end_layout

\begin_layout Theorem
Consider a variational approximation 
\begin_inset Formula $\qtthetapost\left(\theta\right)$
\end_inset

 given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:perturbed_vb_approximation"

\end_inset

 to the perturbed posterior 
\begin_inset Formula $\ptthetapost\left(\theta\right)$
\end_inset

 given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:p_tilting"

\end_inset

.
 Take the expectation of a posterior expectation of 
\begin_inset Formula $\gtheta$
\end_inset

 with respect to 
\begin_inset Formula $\qtthetapost\left(\theta\right)$
\end_inset

.
 Then, under 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:tilt_exists"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:opt_interior"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

, and the definitions given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "def:vb_derivatives"

\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray}
\left.\frac{d\mbe_{\qtthetapost}\left[\gtheta\right]}{dt^{\trans}}\right|_{t=0} & = & \eggrad\klhess^{-1}\efhess^{\trans}.\label{eq:lrvb_formula}
\end{eqnarray}

\end_inset


\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
A proof and the necessary technical conditions are given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:lrvb"

\end_inset

.
 By choosing the appropriate 
\begin_inset Formula $f\left(\theta,t\right)$
\end_inset

 and evaluating 
\begin_inset Formula $\efhess$
\end_inset

, we can use 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 to calculate the exact sensitivity of VB solutions to nearly any arbitrary
 local perturbations that satisfy the regularity conditions.
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_formula"

\end_inset

 is formally similar to frequentist sensitivity estimates.
 For example, the pioneering paper of 
\begin_inset CommandInset citation
LatexCommand citet
key "cook:1986:assessment"

\end_inset

 contains a formula for assessing the curvature of a marginal likelihood
 surface 
\begin_inset CommandInset citation
LatexCommand citep
after "Equation 15"
key "cook:1986:assessment"

\end_inset

 that, like our 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

, represents the sensitivity as a linear system involving the Hessian of
 an objective function at its optimum.
 The geometric interpretation of local robustness suggested by 
\begin_inset CommandInset citation
LatexCommand citet
key "cook:1986:assessment"

\end_inset

 has been extended to Bayesian settings (see, for example, 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu:2007:perturbation,zhu:2011:bayesian"

\end_inset

).
 In addition to generality, one attractive aspect of their geometric approach
 is its invariance to parameterization.
 Investigating geometric interpretations of the present work may be an interesti
ng avenue for future research.
 Additionally, we note that, much as 
\begin_inset CommandInset citation
LatexCommand citet
key "neal:1998:variationalEM"

\end_inset

 view VB as a generalization of the expectation-maximization (EM) algorithm,
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 can be understood as a variational generalization of the 
\begin_inset Quotes eld
\end_inset

structured EM
\begin_inset Quotes erd
\end_inset

 (SEM) covariance estimate of 
\begin_inset CommandInset citation
LatexCommand citet
key "meng:1991:using"

\end_inset

.
 We will elaborate on the connection between 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 and SEM in future work.
\end_layout

\begin_layout Standard
To close this section, we observe that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 is the 
\shape italic
exact sensitivity
\shape default
 of an 
\shape italic
approximate posterior
\shape default
.
 That is, when we can solve 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

, we have used the choice of the family 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 to simplify the problem enough that its local sensitivity to perturbation
 has a closed form.
 In contrast, even when MCMC has converged and is producing draws from the
 exact posterior, the sample covariance estimate in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:mcmc_sample_cov"

\end_inset

 represents the 
\shape italic
approximate sensitivity
\shape default
 of the 
\shape italic
exact posterior
\shape default
.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 is then useful to the extent that the VB posterior mean approximates the
 exact posterior means for all perturbations of interest—that is, to the
 extent that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds.
\end_layout

\begin_layout Section
Calculation and uses of sensitivity
\begin_inset CommandInset label
LatexCommand label
name "sec:sensitivity_in_action"

\end_inset


\end_layout

\begin_layout Standard
In this section, we briefly discuss practical issues involved in the use
 of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

.
 We then apply the results of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:theory"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 with particular choices of the perturbation 
\begin_inset Formula $f\left(\theta;t\right)$
\end_inset

 to calculate covariances and sensitivity measures for VB estimates.
 Throughout this section, we will assume that we can apply 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 unless stated otherwise.
\end_layout

\begin_layout Subsection
Covariances for variational Bayes
\begin_inset CommandInset label
LatexCommand label
name "subsec:lrvb_cov"

\end_inset


\end_layout

\begin_layout Standard
Consider the mean field approximating family, 
\begin_inset Formula $\mathcal{Q}_{mf}$
\end_inset

, from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:variational_Bayes"

\end_inset

.
 It is well known that the resulting marginal variances also tend to be
 under-estimated even when location parameters are well-estimated (see,
 e.g., 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 10"
key "mackay:2003:information,wang:2005:inadequacy,turner:2011:two,bishop:2006:pattern"

\end_inset

).
 Even more obviously, any 
\begin_inset Formula $q\in\mathcal{Q}_{mf}$
\end_inset

 represents as zero the covariance between sub-components of 
\begin_inset Formula $\theta$
\end_inset

 that are in different factors of the mean field approximating family.
 It is therefore unreasonable to expect that 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)\approx\cov_{\pthetapost}\left(\gtheta\right)$
\end_inset

.
 However, if 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds, we may expect the sensitivity of VB means to certain perturbations
 to be accurate, and by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

, we expect the corresponding covariances to be accurately estimated by
 the VB sensitivity.
 In particular, by taking 
\begin_inset Formula $f\left(\theta,t\right)=t^{\trans}g\left(\theta\right)$
\end_inset

 and under 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

, we have
\begin_inset Formula 
\begin{align*}
\left.\frac{d\mbe_{\qtthetapost}\left[\gtheta\right]}{dt^{\trans}}\right|_{t=0} & \approx\left.\frac{d\mbe_{\ptthetapost}\left[\gtheta\right]}{dt^{\trans}}\right|_{t=0}=\cov_{\pthetapost}\left(g\left(\theta\right)\right).
\end{align*}

\end_inset

 Thus, we can use 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 to provide an estimate of 
\begin_inset Formula $\cov_{\pthetapost}\left(g\left(\theta\right)\right)$
\end_inset

 that may be superior to 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)$
\end_inset

.
 With this motivation in mind, we make the following definition.
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:lrvb_covariance"

\end_inset


\end_layout

\begin_layout Definition
The 
\shape italic
linear response approximation
\shape default
, 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

, to the exact posterior covariance 
\begin_inset Formula $\cov_{\pthetapost}\left(\gtheta\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{align}
\lrvbcov\left(\gtheta\right) & :=\eggrad\klhess^{-1}\eggrad^{\trans}\approx\cov_{\pthetapost}\left(\gtheta\right).\label{eq:lrvb_for_covariance}
\end{align}

\end_inset


\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
When 
\begin_inset Formula $\etaopt$
\end_inset

 is a strict local minimum of 
\begin_inset Formula $KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)$
\end_inset

, then 
\begin_inset Formula $\klhess$
\end_inset

 will be positive definite and symmetric, and, as desired, the covariance
 estimate 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 will be as well.
 Since the optimal value of every component of 
\begin_inset Formula $\mbe_{\qthetapost}\left[\gtheta\right]$
\end_inset

 may be affected by the log perturbation 
\begin_inset Formula $t^{\trans}\gtheta$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:p_tilting"

\end_inset

, 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 can estimate non-zero covariances between elements of 
\begin_inset Formula $\gtheta$
\end_inset

 even when they have been partitioned into separate factors of the mean
 field approximation.
 
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 and 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)$
\end_inset

 differ only when there are at least some moments of 
\begin_inset Formula $\pthetapost$
\end_inset

 that 
\begin_inset Formula $\qthetapost$
\end_inset

 fails to accurately estimate.
 In particular, if 
\begin_inset Formula $\qthetapost$
\end_inset

 provided a good approximation to 
\begin_inset Formula $\pthetapost$
\end_inset

 for both the first and second moments of 
\begin_inset Formula $\gtheta$
\end_inset

, then we would have 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)\approx\cov_{\qthetapost}\left(g\left(\theta\right)\right)$
\end_inset

 since
\begin_inset Formula 
\begin{align*}
\mbeq\left[\gtheta\right]\approx\epgtheta\textrm{ and }\\
\mbeq\left[\gtheta\gtheta^{\trans}\right]\approx\mbep\left[\gtheta\gtheta^{\trans}\right] & \Rightarrow\cov_{\qthetapost}\left(g\left(\theta\right)\right)\approx\cov_{\pthetapost}\left(\gtheta\right)\\
\mbeq\left[\gtheta\right]\approx\epgtheta & \Rightarrow\lrvbcov\left(\gtheta\right)\approx\cov_{\pthetapost}\left(\gtheta\right).
\end{align*}

\end_inset

Putting these together, we see that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mbeq\left[\gtheta\right]\approx\epgtheta\textrm{ and }\\
\mbeq\left[\gtheta\gtheta^{\trans}\right]\approx\mbep\left[\gtheta\gtheta^{\trans}\right] & \Rightarrow\cov_{\qthetapost}\left(g\left(\theta\right)\right)\approx\lrvbcov\left(\gtheta\right).
\end{align*}

\end_inset

However, in general, 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)\ne\cov_{\qthetapost}\left(g\left(\theta\right)\right)$
\end_inset

.
 In this sense, any discrepancy between 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 and 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)$
\end_inset

 indicates an inadequacy of the variational approximation for at least the
 second moments of 
\begin_inset Formula $\gtheta$
\end_inset

.
\end_layout

\begin_layout Standard
Let us consider a simple concrete illustrative example which will demonstrate
 how 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)$
\end_inset

 can be a poor approximation to 
\begin_inset Formula $\textrm{Cov}_{\pthetapost}\left(\gtheta\right)$
\end_inset

 and how 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 can improve the approximation for some moments but not others.
 Suppose that the exact posterior is a bivariate normal,
\begin_inset Formula 
\begin{align}
\pthetapost\left(\theta\right) & =\mathcal{N}\left(\theta;\mu,\covmat\right),\label{eq:normal_example}
\end{align}

\end_inset

where 
\begin_inset Formula $\theta=\left(\theta_{1},\theta_{2}\right)^{\trans}$
\end_inset

, 
\begin_inset Formula $\mu=\left(\mu_{1},\mu_{2}\right)^{\trans}$
\end_inset

, 
\begin_inset Formula $\covmat$
\end_inset

 is invertible, and 
\begin_inset Formula $\infomat:=\covmat^{-1}$
\end_inset

.
 One may think of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\covmat$
\end_inset

 as known functions of 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 via Bayes' theorem, for example, as given by a normal-normal conjugate
 model.
 Suppose we use the MFVB approximating family
\begin_inset Formula 
\begin{align*}
\mathcal{Q}_{mf} & =\left\{ q\left(\theta\right):q\left(\theta\right)=q\left(\theta_{1}\right)q\left(\theta_{2}\right)\right\} .
\end{align*}

\end_inset

It is not hard to show (see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:mvn_exact"

\end_inset

) that the optimal VB approximation to 
\begin_inset Formula $\pthetapost$
\end_inset

 in the family 
\begin_inset Formula $\mathcal{Q}_{mf}$
\end_inset

 is given by
\begin_inset Formula 
\begin{align*}
q\left(\theta_{1}\right) & =\mathcal{N}\left(\theta_{1};\mu_{1},\infomat_{11}^{-1}\right)\\
q\left(\theta_{2}\right) & =\mathcal{N}\left(\theta_{2};\mu_{2},\infomat_{22}^{-1}\right).
\end{align*}

\end_inset

Note that the posterior mean of 
\begin_inset Formula $\theta_{1}$
\end_inset

 is exactly estimated by the MFVB procedure:
\begin_inset Formula 
\begin{align*}
\mbeq\left[\theta_{1}\right] & =\mu_{1}=\mbep\left[\theta_{1}\right].
\end{align*}

\end_inset

However, if 
\begin_inset Formula $\covmat_{12}\ne0$
\end_inset

, then 
\begin_inset Formula $\infomat_{11}^{-1}<\covmat_{11}$
\end_inset

, and the variance of 
\begin_inset Formula $\theta_{1}$
\end_inset

 is underestimated.
 This means that the expectation of 
\begin_inset Formula $\theta_{1}^{2}$
\end_inset

 is 
\shape italic
not
\shape default
 correctly estimated by the MFVB procedure:
\begin_inset Formula 
\begin{align*}
\mbeq\left[\theta_{1}^{2}\right] & =\mu_{1}^{2}+\infomat_{11}^{-1}\ne\mu_{1}^{2}+\covmat_{11}=\mbep\left[\theta_{1}^{2}\right].
\end{align*}

\end_inset

A similar analysis holds for 
\begin_inset Formula $\theta_{2}$
\end_inset

.
 Of course, the covariance is also mis-estimated if 
\begin_inset Formula $\covmat_{12}\ne0$
\end_inset

 since, by construction of the MFVB approximation,
\begin_inset Formula 
\begin{align*}
\cov_{\qthetapost}\left(\theta_{1},\theta_{2}\right) & =0\ne\covmat_{12}=\cov_{\pthetapost}\left(\theta_{1},\theta_{2}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Now let us take 
\begin_inset Formula $f\left(\theta,t\right)=\theta_{1}t_{1}+\theta_{2}t_{2}$
\end_inset

.
 For all 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero, the perturbed posterior given by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:p_tilting"

\end_inset

 remains multivariate normal, so it remains the case that, as a function
 of 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\mbe_{\qtthetapost}\left[\theta_{1}\right]=\mbe_{\ptthetapost}\left[\theta_{1}\right]$
\end_inset

 and 
\begin_inset Formula $\mbe_{\qtthetapost}\left[\theta_{2}\right]=\mbe_{\ptthetapost}\left[\theta_{2}\right]$
\end_inset

.
 Consequently, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds with exact equalities when 
\begin_inset Formula $\gtheta=\theta$
\end_inset

.
 However, since the second moments are not accurate (irrespective of 
\begin_inset Formula $t$
\end_inset

), 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 does not hold with exact equalities when 
\begin_inset Formula $\gtheta=\left(\theta_{1}^{2},\theta_{2}^{2}\right)^{\trans}$
\end_inset

, nor when 
\begin_inset Formula $\gtheta=\theta_{1}\theta_{2}$
\end_inset

.
 (
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 may still hold approximately for second moments when 
\begin_inset Formula $\covmat_{12}$
\end_inset

 is small.) The fact that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds with exact equalities for 
\begin_inset Formula $\gtheta=\theta$
\end_inset

 allows us to use 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 to calculate 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)=\cov_{\pthetapost}\left(g\left(\theta\right)\right)$
\end_inset

, even though 
\begin_inset Formula $\mbe_{\pthetapost}\left[\theta_{1}\theta_{2}\right]$
\end_inset

 and 
\begin_inset Formula $\mbe_{\pthetapost}\left[\left(\theta_{1}^{2},\theta_{2}^{2}\right)^{\trans}\right]$
\end_inset

 are mis-estimated.
\end_layout

\begin_layout Standard
In fact, when 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds with exact equalities for some 
\begin_inset Formula $\theta_{i}$
\end_inset

 (as in the multivariate Gaussian case just discussed), then the estimated
 covariance in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_for_covariance"

\end_inset

 for all terms involving 
\begin_inset Formula $\theta_{i}$
\end_inset

 will be exact as well.
 This is the case for the bivariate normal model above, and described in
 detail for the general multivariate normal case in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:mvn_exact"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_for_covariance"

\end_inset

 is a generalization to arbitrary variational approximations of results
 from 
\begin_inset CommandInset citation
LatexCommand citet
key "giordano:2015:lrvb"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "giordano:2015:lrvb"

\end_inset

 also demonstrate the effectiveness of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_for_covariance"

\end_inset

 on a range of practical problems.
 Below, in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

, in addition to robustness measures, we will also report the accuracy of
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_for_covariance"

\end_inset

 for estimating posterior covariances.
 We find that, for most parameters of interest, particularly location parameters
, 
\begin_inset Formula $\lrvbcov\left(g\left(\theta\right)\right)$
\end_inset

 provides a good approximation to 
\begin_inset Formula $\cov_{\pthetapost}\left(g\left(\theta\right)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The application of sensitivity measures to VB problems for the purpose of
 improving covariance estimates has a long history under the name 
\begin_inset Quotes eld
\end_inset

linear response methods.
\begin_inset Quotes erd
\end_inset

 These methods originated in the statistical physics literature 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g."
key "tanaka:2000:information,opper:2001:advancedmeanfield"

\end_inset

 and have been applied to various statistical and machine learning problems
 
\begin_inset CommandInset citation
LatexCommand citep
key "kappen:1998:efficient,tanaka:1998:mean,welling:2004:linear,opper:2003:variational"

\end_inset

.
 Our work, which builds on our earlier work in 
\begin_inset CommandInset citation
LatexCommand citet
key "giordano:2015:lrvb"

\end_inset

, represents a simplification and generalization of classical linear response
 methods, and serves to elucidate the relationship between these methods
 and the local robustness literature.
 In particular, while 
\begin_inset CommandInset citation
LatexCommand citet
key "giordano:2015:lrvb"

\end_inset

 focused on moment-parameterized exponential families, we derive linear-response
 covariances for generic variational approximations and connect the linear-respo
nse methodology to the Bayesian robustness literature.
\end_layout

\begin_layout Standard
A very reasonable approach to the inadequacy of VB covariances is to simply
 increase the expressiveness of the model class 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 (although, as noted by 
\begin_inset CommandInset citation
LatexCommand citet
key "turner:2011:two"

\end_inset

, increased expressiveness does not necessarily lead to better solutions).
 This is the approach taken by much of the recent VB literature 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g."
key "ranganath:201:4black,tran:2015:copula,tran:2015:gp,ranganath:2015:hierarchical,liu:2016:stein"

\end_inset

.
 Though this remains a lively and promising research direction, the use
 of a more complex class 
\begin_inset Formula $\mathcal{Q}$
\end_inset

 sometimes sacrifices the speed and simplicity that made VB attractive in
 the first place, and often without the relatively well-understood convergence
 guarantees of MCMC.
 We also stress that the current work is not at necessarily at odds with
 the approach of increasing expressiveness.
 Sensitivity methods can be a supplement to any VB approximation for which
 our estimators (which require solving a linear system involving the Hessian
 of the KL divergence) are tractable.
\end_layout

\begin_layout Subsection
Local prior sensitivity for VB 
\begin_inset CommandInset label
LatexCommand label
name "subsec:lrvb_robustness"

\end_inset

 
\end_layout

\begin_layout Standard
We now turn to estimating prior sensitivity for VB estimates.
 
\end_layout

\begin_layout Definition
The 
\shape italic
local sensitivity
\shape default
 of 
\begin_inset Formula $\eqgtheta$
\end_inset

 to prior parameter 
\begin_inset Formula $\alpha$
\end_inset

 is given by
\begin_inset Formula 
\begin{align*}
\qsens & :=\left.\frac{d\eqgtheta}{d\alpha}\right|_{\alpha}.
\end{align*}

\end_inset


\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Under 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

, 
\begin_inset Formula $\qsens\approx\psens$
\end_inset

.
 We now to turn to finding a form of 
\begin_inset Formula $f\left(\theta,t\right)$
\end_inset

 for 
\begin_inset CommandInset ref
LatexCommand formatted
reference "def:vb_derivatives"

\end_inset

 to produce 
\begin_inset Formula $\qsens$
\end_inset

.
 Under 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:bayes_ok"

\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:sens_and_cov"

\end_inset

, 
\begin_inset Formula $\log\prior$
\end_inset

 is smooth in 
\begin_inset Formula $\alpha$
\end_inset

 and well-defined in a neighborhood of 
\begin_inset Formula $\alpha$
\end_inset

.
 Choose a small 
\begin_inset Formula $t$
\end_inset

 having the same dimension as 
\begin_inset Formula $\alpha$
\end_inset

.
 Then, up to a constant that captures log-normalizing constants that do
 not depend on 
\begin_inset Formula $\theta$
\end_inset

 (in an abuse of notation, the values of 
\begin_inset Formula $\constant$
\end_inset

 are different from line to line), we have the log posterior for a slightly
 different 
\begin_inset Formula $\alpha$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\log\pthetapost[\alpha+t]\left(\theta\right) & := & \log p\left(x\vert\theta\right)+\log\prior[][\alpha+t]+\constant\\
 & = & \log p\left(x\vert\theta\right)+\log\prior[][\alpha]+\left(\log\prior[][\alpha+t]-\log\prior[][\alpha]\right)+\constant\\
 & = & \log\pthetapost\left(\theta\right)+\left(\log\prior[][\alpha+t]-\log\prior[][\alpha]\right)+\constant.
\end{eqnarray*}

\end_inset

Here, if we take
\begin_inset Formula 
\begin{align*}
f\left(\theta,t\right) & :=\log\prior[][\alpha+t]-\log\prior[][\alpha],
\end{align*}

\end_inset

we can see that 
\begin_inset Formula $\efhess$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "def:vb_derivatives"

\end_inset

 is
\begin_inset Formula 
\begin{eqnarray}
\efhess & = & \left.\frac{\partial^{2}\mbeq\left[\log p\left(\theta\vert\alpha+t\right)\right]}{\partial t\partial\eta^{\trans}}\right|_{\eta=\etaopt,t=0}=\left.\frac{\partial}{\partial\eta^{\trans}}\mbeq\left[\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha}\right]\right|_{\eta=\etaopt,\alpha}.\label{eq:lrvb_parametric_sens}
\end{eqnarray}

\end_inset

We can thus calculate the variational prior sensitivity:
\begin_inset Formula 
\begin{align}
\qsens & =\eggrad\klhess^{-1}\left.\frac{\partial}{\partial\eta^{\trans}}\mbeq\left[\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha}\right]\right|_{\eta=\etaopt,\alpha}.\label{eq:variational_local_robustness}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Note that—up to the numerical task of calculating the terms in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:variational_local_robustness"

\end_inset

—
\begin_inset Formula $\qsens$
\end_inset

 is the exact sensitivity of the variational mean 
\begin_inset Formula $\eqgtheta$
\end_inset

.
 Again, it is the exact sensitivity of an approximate posterior.
 It is an approximation to 
\begin_inset Formula $\psens$
\end_inset

 to the extent that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds—that is, to the extent that the VB means are good approximations
 to the exact means.
 We now use 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:variational_local_robustness"

\end_inset

 to reproduce VB versions of some standard robustness measures found in
 the existing literature.
\end_layout

\begin_layout Subsection
Parametric sensitivity
\begin_inset CommandInset label
LatexCommand label
name "subsec:Parametric-sensitivity"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lkjparam}{\boldsymbol{\Psi}}
{\boldsymbol{\Psi}}
\end_inset


\end_layout

\begin_layout Standard
A simple case is when the prior 
\begin_inset Formula $\prior$
\end_inset

 is believed to be in a given parametric family, and we are simply interested
 in the effect of varying the parametric family's parameters 
\begin_inset CommandInset citation
LatexCommand citep
key "basu:1996:local"

\end_inset

.
 We will refer to this kind of perturbation as a 
\begin_inset Quotes eld
\end_inset

parametric sensitivity,
\begin_inset Quotes erd
\end_inset

 in contrast to 
\begin_inset Quotes eld
\end_inset

functional sensitivity,
\begin_inset Quotes erd
\end_inset

 which we discuss in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:functional_sensitivity"

\end_inset

.
\end_layout

\begin_layout Standard
For illustration, we first consider a simple example where 
\begin_inset Formula $p\left(\theta\vert\alpha\right)$
\end_inset

 is in the exponential family, with natural sufficient statistic 
\begin_inset Formula $\theta$
\end_inset

 and log normalizer 
\begin_inset Formula $A\left(\alpha\right)$
\end_inset

, and we take 
\begin_inset Formula $g\left(\theta\right)=\theta$
\end_inset

.
 In this case,
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(\theta\vert\alpha\right) & = & \alpha^{\trans}\theta-A\left(\alpha\right)\\
\efhess & = & \left.\frac{\partial}{\partial\eta^{\trans}}\mbeq\left[\frac{\partial}{\partial\alpha}\left(\alpha^{\trans}\theta-A\left(\alpha\right)\right)\right]\right|_{\eta=\etaopt,\alpha}\\
 & = & \left.\frac{\partial}{\partial\eta^{\trans}}\mbeq\left[\theta\right]-\frac{\partial}{\partial\eta^{\trans}}\frac{\partial A\left(\alpha\right)}{\partial\alpha}\right|_{\eta=\etaopt,\alpha}\\
 & = & \eggrad.
\end{eqnarray*}

\end_inset

Recognizing that, when 
\begin_inset Formula $\efhess=\eggrad$
\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:variational_local_robustness"

\end_inset

 is equivalent to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_for_covariance"

\end_inset

, we see that
\begin_inset Formula 
\begin{align*}
\qsens & =\lrvbcov\left(\theta\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, the sensitivity is simply the linear response covariance estimate
 of the covariance, 
\begin_inset Formula $\lrvbcov\left(\theta\right)$
\end_inset

.
 Following the same reasoning, the exact posterior sensitivity is given
 by
\begin_inset Formula 
\begin{align*}
\psens & =\cov_{\pthetapost}\left(\theta\right).
\end{align*}

\end_inset

Thus, 
\begin_inset Formula $\qsens\approx\psens$
\end_inset

 to the extent that 
\begin_inset Formula $\lrvbcov\left(\theta\right)\approx\cov_{\pthetapost}\left(\theta\right)$
\end_inset

, which again holds to the extent that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 holds.
 Note that if we had used a mean field assumption and had tried to use the
 direct, uncorrected response covariance 
\begin_inset Formula $\cov_{\qthetapost}\left(\theta\right)$
\end_inset

 to try to evaluate 
\begin_inset Formula $\qsens$
\end_inset

, we would have erroneously concluded that the prior on one component, 
\begin_inset Formula $\theta_{k_{1}}$
\end_inset

, would not affect the posterior mean of some other component, 
\begin_inset Formula $\theta_{k_{2}}$
\end_inset

, for 
\begin_inset Formula $k_{2}\ne k_{1}$
\end_inset

.
\end_layout

\begin_layout Standard
Sometimes it is easy to evaluate the derivative of the log prior even when
 it is not easy to normalize it.
 For example, as we show in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:lkj"

\end_inset

, the LKJ covariance prior 
\begin_inset CommandInset citation
LatexCommand citep
key "lewandowski:2009:lkj"

\end_inset

 has a closed-form log expectation when using an inverse Wishart variational
 approximation.
 Let 
\begin_inset Formula $\covmat$
\end_inset

 be an unknown 
\begin_inset Formula $K\times K$
\end_inset

 information matrix (i.e., the inverse of a covariance matrix that is part
 of 
\begin_inset Formula $\theta$
\end_inset

).
 Let
\begin_inset Formula 
\begin{align*}
q\left(\covmat\right) & :=\textrm{InverseWishart}\left(\covmat\vert\lkjparam,\nu\right)\\
p\left(\covmat\vert\alpha\right) & \propto\textrm{LKJ}\left(\covmat\vert\alpha\right),
\end{align*}

\end_inset

where 
\begin_inset Formula $\lkjparam$
\end_inset

 is a positive definite scale matrix, 
\begin_inset Formula $\nu$
\end_inset

 is the degrees of freedom, and 
\begin_inset Formula $\textrm{LKJ}\left(\alpha\right)$
\end_inset

 is a prior on correlation matrices with concentration parameters 
\begin_inset Formula $\alpha$
\end_inset

.
 For illustration, we will show how to calculate the local sensitivity to
 the LKJ concentration parameter.
 Since we take the partial derivative with respect to 
\begin_inset Formula $\alpha$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:variational_local_robustness"

\end_inset

, we can omit terms from the prior that do not depend on 
\begin_inset Formula $\alpha$
\end_inset

 (e.g.
 any priors on the diagonal of 
\begin_inset Formula $\covmat$
\end_inset

, which we assume are in the constant of proportionality in the definition
 of 
\begin_inset Formula $p\left(\covmat\vert\alpha\right)$
\end_inset

 and are independent of 
\begin_inset Formula $\alpha$
\end_inset

).
 In this case, the variational parameters are 
\begin_inset Formula $\eta=\left(\lkjparam,\nu\right)$
\end_inset

, with the understanding that we have stacked only the upper-diagonal elements
 of 
\begin_inset Formula $\lkjparam$
\end_inset

, since 
\begin_inset Formula $\lkjparam$
\end_inset

 is constrained to be symmetric and 
\begin_inset Formula $\etaopt$
\end_inset

 must be interior.
 As we show in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:lkj"

\end_inset

,
\begin_inset Formula 
\begin{align*}
\mbeq\left[\log p\left(\covmat\vert\alpha\right)\right] & =\left(\alpha-1\right)\left(\log\left|\lkjparam\right|-\psi_{K}\left(\frac{\nu}{2}\right)-\sum_{k=1}^{K}\log\left(\frac{1}{2}\lkjparam_{kk}\right)-K\psi\left(\frac{\nu-K+1}{2}\right)\right)+\constant,
\end{align*}

\end_inset

where 
\begin_inset Formula $\constant$
\end_inset

 contains terms that do not depend on 
\begin_inset Formula $\alpha$
\end_inset

.
 Here, 
\begin_inset Formula $\psi_{K}$
\end_inset

 denotes the multivariate digamma function.
 Consequently, we can evaluate
\begin_inset Formula 
\begin{align}
\efhess=\frac{\partial}{\partial\eta^{\trans}}\mbeq\left[\frac{\partial}{\partial\alpha}\log p\left(\covmat\vert\alpha\right)\right] & =\frac{\partial}{\partial\eta^{\trans}}\left(\log\left|\lkjparam\right|-\psi_{K}\left(\frac{n}{2}\right)-\sum_{k=1}^{K}\log\left(\frac{1}{2}\lkjparam_{kk}\right)-K\psi\left(\frac{n-K+1}{2}\right)\right).\label{eq:lkj_prior_vb}
\end{align}

\end_inset

This derivative has a closed form, though the bookkeeping required to represent
 an unconstrained parameterization of the matrix 
\begin_inset Formula $\lkjparam$
\end_inset

 within 
\begin_inset Formula $\eta$
\end_inset

 would be tedious.
 In practice, we evaluate terms like 
\begin_inset Formula $\efhess$
\end_inset

 using automatic differentiation tools from the Python 
\family typewriter
\shape italic
autograd
\family default
\shape default
 library 
\begin_inset CommandInset citation
LatexCommand citep
key "maclaurin:2015:autograd"

\end_inset

.
\end_layout

\begin_layout Standard
Finally, in cases where we cannot evaluate 
\begin_inset Formula $\mbeq\left[\log\prior\right]$
\end_inset

 in closed form as a function of 
\begin_inset Formula $\eta$
\end_inset

, we can use numerical techniques as described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:lrvb_implementation"

\end_inset

.
 We thus view 
\begin_inset Formula $\qsens$
\end_inset

 as the exact sensitivity to an approximate KL divergence.
\end_layout

\begin_layout Subsection
Functional sensitivity
\begin_inset CommandInset label
LatexCommand label
name "subsec:functional_sensitivity"

\end_inset


\end_layout

\begin_layout Standard
The parametric perturbations described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Parametric-sensitivity"

\end_inset

 may be insufficiently expressive to describe the full range of plausible
 priors.
 Parametric priors are often chosen merely for analytic tractability (e.g.,
 conjugacy) even when much more general functional forms would be subjectively
 plausible.
 In this section, we describe the effect of perturbing the prior additively
 with arbitrary functions, which we refer to as 
\begin_inset Quotes eld
\end_inset

functional sensitivity.
\begin_inset Quotes erd
\end_inset

 In the present work, we will assume that the practitioner has a particular
 functional perturbation in mind.
 However, starting from the results of this section, a VB version of the
 influence function and worst-case functional perturbation analysis of 
\begin_inset CommandInset citation
LatexCommand citet
key "gustafson:1996:localposterior"

\end_inset

 follows naturally, though we leave the detailed development of these ideas
 for future work.
\end_layout

\begin_layout Standard
In order to evaluate the effect of changing the prior's functional form,
 we consider adding to our original prior, 
\begin_inset Formula $\origprior$
\end_inset

, a weighted non-negative and 
\begin_inset Formula $\lambda$
\end_inset

-integrable measure 
\begin_inset Formula $\contamprior$
\end_inset

.
 After normalizing, we get
\begin_inset Formula 
\begin{eqnarray}
\prior & = & \frac{\origprior+\alpha\contamprior}{\int\left(\origprior[\theta']+\alpha\contamprior[\theta']\right)\lambda\left(d\theta'\right)}=\frac{\origprior+\alpha\contamprior}{1+\alpha\contamnorm}\textrm{ for }\alpha\ge0,\label{eq:u_contamination}
\end{eqnarray}

\end_inset

where we have defined 
\begin_inset Formula $\contamnorm:=\int\contamprior\lambda\left(d\theta\right)$
\end_inset

.
 We will always consider local sensitivity at 
\begin_inset Formula $\alpha=0$
\end_inset

; i.e., we consider local additive perturbations to 
\begin_inset Formula $\origprior$
\end_inset

 using approximations to the posterior distribution with 
\begin_inset Formula $\prior[][0]=\origprior$
\end_inset

.
 
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\contamprior$
\end_inset

 need not be a probability distribution; i.e., we may have 
\begin_inset Formula $\contamnorm\ne1$
\end_inset

.
 When 
\begin_inset Formula $\contamnorm=1$
\end_inset

, then 
\begin_inset Formula $\prior$
\end_inset

 is a mixture between the two distributions 
\begin_inset Formula $\origprior$
\end_inset

 and 
\begin_inset Formula $\contamprior$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\prior= & \left(1-\epsilon\right)\origprior+\epsilon\contamprior\textrm{ for }\epsilon:=\frac{\alpha}{1+\alpha}.
\end{align*}

\end_inset

In this case, the perturbation described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

 is known as 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\epsilon$
\end_inset

-contamination
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "gustafson:2012:localrobustnessbook"

\end_inset

.
 Although we differentiate with respect to 
\begin_inset Formula $\alpha$
\end_inset

 instead of 
\begin_inset Formula $\epsilon$
\end_inset

, since 
\begin_inset Formula $\left.\frac{d\epsilon}{d\alpha}\right|_{\alpha=0}=1$
\end_inset

, the local sensitivity at 
\begin_inset Formula $\origprior$
\end_inset

 is the same for both parameterizations.
 We do not always assume that 
\begin_inset Formula $\contamnorm=1$
\end_inset

, however, since the worst-case perturbation subject to being within a certain
 distance of 
\begin_inset Formula $\origprior$
\end_inset

 may not be a probability distribution 
\begin_inset CommandInset citation
LatexCommand citep
key "gustafson:1996:localposterior"

\end_inset

.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

, 
\begin_inset Formula $\alpha$
\end_inset

 is a single non-negative scalar, not a function.
 In other words, for the purpose of evaluating 
\begin_inset Formula $\psens$
\end_inset

 we keep 
\begin_inset Formula $\contamprior$
\end_inset

 fixed, though it is useful to remember that 
\begin_inset Formula $\prior$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

 and 
\begin_inset Formula $\psens$
\end_inset

 are both functionals of 
\begin_inset Formula $\contamprior$
\end_inset

.
 Furthermore, for notational simplicity, will assume that the quantity of
 interest, 
\begin_inset Formula $\gtheta$
\end_inset

, is a scalar-valued function when discussing functional sensitivity.
 To emphasize these points, we make the following definition.
\end_layout

\begin_layout Definition
When the prior perturbation is of the form 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

, and 
\begin_inset Formula $\gtheta$
\end_inset

 is a scalar-valued function, we define
\begin_inset Formula 
\begin{align*}
\psensfunc:=\psens & \quad\textrm{and}\quad\qsensfunc:=\qsens.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Viewed as a functional of the perturbation 
\begin_inset Formula $\contamprior$
\end_inset

, derivatives with respect to 
\begin_inset Formula $\alpha$
\end_inset

 become Gateaux derivatives of 
\begin_inset Formula $\epgtheta$
\end_inset

 and 
\begin_inset Formula $\eqgtheta$
\end_inset

 in the direction of 
\begin_inset Formula $\contamprior$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
after "Section 2.5"
key "huber:2011:robust"

\end_inset

.
\end_layout

\begin_layout Standard
Assuming that 
\begin_inset Formula $\contamprior$
\end_inset

 is sufficiently well behaved, we can apply 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:sens_cov_prior"

\end_inset

 to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:mcmc_epsilon_sensitivity"

\end_inset

Assume a given posterior 
\begin_inset Formula $\pthetapost$
\end_inset


\begin_inset Formula $\left(\theta\right)$
\end_inset

, a function of interest 
\begin_inset Formula $\gtheta$
\end_inset

, and an original prior 
\begin_inset Formula $\origprior$
\end_inset

.
 Consider the family of prior perturbations given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

.
 Given that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

 holds for our particular 
\begin_inset Formula $\gtheta$
\end_inset

, then
\begin_inset Formula 
\begin{align}
\psensfunc & :=\psens=\cov_{\pthetapost}\left(g\left(\theta\right),\frac{\contamprior}{\origprior}\right).\label{eq:mcmc_epsilon_sensitivity}
\end{align}

\end_inset


\end_layout

\begin_layout Proof
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:bayes_ok"

\end_inset

 will be satisfied since 
\begin_inset Formula $\contamprior$
\end_inset

 is integrable, and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

 holds by assumption, so we can apply 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

.
 By direct calculation, 
\begin_inset Formula 
\begin{align}
\left.\frac{\partial\log\prior}{\partial\alpha}\right|_{\alpha=0} & =\frac{\contamprior}{\origprior}-\contamnorm.\label{eq:log_prior_deriv}
\end{align}

\end_inset

Since covariances are not affected by mean shifts, the conclusion follows.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:mcmc_epsilon_sensitivity"

\end_inset

 is equivalent to 
\begin_inset CommandInset citation
LatexCommand citet
after "Result 8"
key "gustafson:1996:localmarginals"

\end_inset

, and derived under similar assumptions.
 We can also derive a variational version of 
\begin_inset Formula $\psensfunc$
\end_inset

, which we denote 
\begin_inset Formula $\qsensfunc$
\end_inset

.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:vb_epsilon_sensitivity"

\end_inset

Assume a given posterior 
\begin_inset Formula $\pthetapost$
\end_inset


\begin_inset Formula $\left(\theta\right)$
\end_inset

, a function of interest 
\begin_inset Formula $\gtheta$
\end_inset

, and an original prior 
\begin_inset Formula $\origprior$
\end_inset

.
 Consider the family of prior perturbations given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:u_contamination"

\end_inset

.
 Given 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:tilt_exists"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:opt_interior"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

, then
\begin_inset Formula 
\begin{eqnarray}
\qsensfunc & = & \eggrad\klhess^{-1}\frac{\partial}{\partial\eta}\mbe_{\qthetapost}\left[\frac{\contamprior}{\origprior}\right].\label{eq:vb_epsilon_sensitivity}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Proof
Under the given assumptions we can apply 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_parametric_sens"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:variational_local_robustness"

\end_inset

, directly.
 We can calculate 
\begin_inset Formula $\efhess$
\end_inset

 using 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:log_prior_deriv"

\end_inset

, noting that 
\begin_inset Formula $\contamprior$
\end_inset

 is fixed so 
\begin_inset Formula $\contamnorm$
\end_inset

 does not depend on 
\begin_inset Formula $\eta$
\end_inset

.
\end_layout

\begin_layout Standard
The necessary assumptions for the VB sensitivity of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:vb_epsilon_sensitivity"

\end_inset

 are not as easily satisfied as those for the exact sensitivity of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:mcmc_epsilon_sensitivity"

\end_inset

.
 For example, note that if 
\begin_inset Formula $\qthetapost\left(\theta\right)$
\end_inset

 has heavier tails than 
\begin_inset Formula $\origprior$
\end_inset

, then the expectation in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:vb_epsilon_sensitivity"

\end_inset

 may be infinite, and, correspondingly, the validity of both 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

 will be doubtful.
\end_layout

\begin_layout Subsection
Practical considerations and benefits of computing the sensitivity of variationa
l approximations
\begin_inset CommandInset label
LatexCommand label
name "subsec:lrvb_implementation"

\end_inset


\end_layout

\begin_layout Standard
We briefly discuss practical issues involved in the computation of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_formula"

\end_inset

, which involves calculating the product 
\begin_inset Formula $\eggrad\klhess^{-1}$
\end_inset

 (or, equivalently, 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 since 
\begin_inset Formula $\klhess$
\end_inset

 is symmetric).
 Calculating 
\begin_inset Formula $\klhess$
\end_inset

 and solving this linear system can be the most computationally intensive
 part of computing 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_formula"

\end_inset

.
 
\end_layout

\begin_layout Standard
We first note that it can be difficult and time consuming in practice to
 manually derive and implement second-order derivatives.
 Even a small programming error can lead to large errors in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

.
 To ensure accuracy and save analyst time, we evaluated all the requisite
 derivatives using the Python 
\family typewriter
autograd
\family default
 automatic differentiation library 
\begin_inset CommandInset citation
LatexCommand citep
key "maclaurin:2015:autograd"

\end_inset

.
\end_layout

\begin_layout Standard
Note that the dimension of 
\begin_inset Formula $\klhess$
\end_inset

 is as large as that of 
\begin_inset Formula $\eta$
\end_inset

, the parameters that specify the variational distribution 
\begin_inset Formula $q\left(\theta;\eta\right)$
\end_inset

.
 Many applications of VB employ many latent variables, the number of which
 may even scale with the amount of data (e.g., the model we examine in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

).
 However, these applications typically have special structure that render
 
\begin_inset Formula $\klhess$
\end_inset

 sparse, allowing the practitioner to calculate 
\begin_inset Formula $\eggrad\klhess^{-1}$
\end_inset

 quickly.
 Consider, for example, a model with 
\begin_inset Quotes eld
\end_inset

global
\begin_inset Quotes erd
\end_inset

 parameters, 
\begin_inset Formula $\theta_{glob}$
\end_inset

, that are shared by all the individual datapoint likelihoods, and 
\begin_inset Quotes eld
\end_inset

local
\begin_inset Quotes erd
\end_inset

 parameters, 
\begin_inset Formula $\theta_{loc,n}$
\end_inset

, associated with likelihood of a single datapoint indexed by 
\begin_inset Formula $n$
\end_inset

.
 By 
\begin_inset Quotes eld
\end_inset

global
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

local
\begin_inset Quotes erd
\end_inset

 we mean the likelihood and assumed variational distribution factorize as
\begin_inset Formula 
\begin{align}
p\left(x,\theta_{glob},\theta_{loc,1},...,\theta_{loc,N}\right) & =p\left(\theta_{glob}\right)\prod_{n=1}^{N}p\left(x\vert\theta_{loc,n},\theta_{glob}\right)p\left(\theta_{loc,n}\vert\theta_{glob}\right)\label{eq:global_local}\\
q\left(\theta;\eta\right) & =q\left(\theta_{glob};\eta_{glob}\right)\prod_{n=1}^{N}q\left(\theta_{loc,n};\eta_{n}\right)\textrm{ for all }q\left(\theta;\eta\right)\in\mathcal{Q}.\nonumber 
\end{align}

\end_inset

In this case, the second derivatives of the variational objective between
 the parameters for local variables vanish:
\begin_inset Formula 
\begin{align*}
\textrm{ for all }n\ne m\textrm{, }\frac{\partial^{2}KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)}{\partial\eta_{loc,n}\partial\eta_{loc,m}^{\trans}} & =0.
\end{align*}

\end_inset

The model in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

 has such a global or local structure; see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_inference"

\end_inset

 for more details.
 Additional discussion, including the use of Schur complements to take advantage
 of sparsity in the log likelihood, can be found in 
\begin_inset CommandInset citation
LatexCommand citet
key "giordano:2015:lrvb"

\end_inset

.
\end_layout

\begin_layout Standard
When even calculating or instantiating 
\begin_inset Formula $\klhess$
\end_inset

 is prohibitively time-consuming, one can use conjugate gradient algorithms
 to approximately compute 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 5"
key "nocedalwright:1999:numerical"

\end_inset

.
 The advantage of conjugate gradient algorithms is that they use only the
 Hessian-vector product 
\begin_inset Formula $\klhess\eggrad^{\trans}$
\end_inset

, which can be computed efficiently using automatic differentiation without
 ever forming the full Hessian 
\begin_inset Formula $\klhess$
\end_inset

 (see, for example, the 
\family typewriter
hessian_vector_product
\family default
 method of the Python 
\family typewriter
autograd
\family default
 package 
\begin_inset CommandInset citation
LatexCommand citep
key "maclaurin:2015:autograd"

\end_inset

).
 Note that a separate conjugate gradient problem must be solved for each
 column of 
\begin_inset Formula $\eggrad^{\trans}$
\end_inset

, so if the parameter of interest 
\begin_inset Formula $\gtheta$
\end_inset

 is high-dimensional it may be faster to pay the price for computing and
 inverting the entire matrix 
\begin_inset Formula $\klhess$
\end_inset

.
 See 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:glmm_inference"

\end_inset

 for more discussion of a specific example.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

, we require 
\begin_inset Formula $\etaopt$
\end_inset

 to be at a true local optimum.
 Otherwise the estimated sensitivities may not be reliable (e.g., the covariance
 implied by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lrvb_for_covariance"

\end_inset

 may not be positive definite).
 We find that the classical VB coordinate ascent algorithms (
\begin_inset CommandInset citation
LatexCommand citet
after "Section 2.4"
key "blei:2016:variational"

\end_inset

) and even quasi-second order methods, such as BFGS 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g."
key "regier:2015:celeste"

\end_inset

, may not actually find a local optimum unless run for a long time with
 very stringent convergence criteria.
 Also, when employing stochastic optimization methods 
\begin_inset CommandInset citation
LatexCommand citep
key "hoffman:2013:stochastic,ranganath:2015:hierarchical"

\end_inset

, the estimated optimum is guaranteed to be only near an optimum, rather
 than at an optimum.
 Consequently, we recommend fitting models using second-order Newton trust
 region methods.
 When the Hessian is slow to compute directly, as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

, one can use the conjugate gradient trust region method of 
\begin_inset CommandInset citation
LatexCommand citet
after " Chapter 7"
key "nocedalwright:1999:numerical"

\end_inset

, which takes advantage of fast automatic differentiation Hessian-vector
 products without forming or inverting the full Hessian.
 
\end_layout

\begin_layout Standard
In the current work, a primary reason for calculating a VB version of local
 sensitivity is to take advantage of VB's speed and scalability.
 However, 
\begin_inset Formula $\qsens$
\end_inset

 and 
\begin_inset Formula $\qsensfunc$
\end_inset

 may have additional benefits over their corresponding MCMC estimates.
 First, sample covariance estimates will naturally be subject to Monte Carlo
 error inherent to MCMC, whereas VB estimates of 
\begin_inset Formula $\psens$
\end_inset

 are typically either analytically tractable (as in, for example, the LKJ
 prior of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lkj_prior_vb"

\end_inset

) or involve only low-dimensional Monte Carlo estimates over known distributions.
 Examples where only relatively easy Monte Carlo estimates are required
 for VB include calculating 
\begin_inset Formula $\eggrad$
\end_inset

 and 
\begin_inset Formula $\efhess$
\end_inset

 using samples from 
\begin_inset Formula $\qthetapost$
\end_inset

 or intractable integrals with respect to 
\begin_inset Formula $q\left(\theta;\eta\right)$
\end_inset

 as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

.
\end_layout

\begin_layout Standard
Finally, the sample covariance estimates versions of the functional sensitivity
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:mcmc_epsilon_sensitivity"

\end_inset

 may have infinite variance.
 To calculate 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:mcmc_epsilon_sensitivity"

\end_inset

 using draws from 
\begin_inset Formula $\pthetapost$
\end_inset


\begin_inset Formula $\left(\theta\right)$
\end_inset

, we will need sample estimates of 
\begin_inset Formula $\mbep\left[\frac{g\left(\theta\right)\contamprior}{\origprior}\right]$
\end_inset

.
 For the variance of this quantity to be finite, we require that the expectation
 of the square is finite.
 The expectation of the square is given by
\begin_inset Formula 
\begin{align*}
\mbep\left[\left(\frac{g\left(\theta\right)\contamprior}{\origprior}\right)^{2}\right] & =\int\frac{\pthetapost\left(\theta\right)}{\origprior^{2}}\left(\gtheta\contamprior\right)^{2}\lambda\left(d\theta\right)\\
 & \propto\int\frac{p\left(x\vert\theta\right)\origprior}{\origprior^{2}}\left(\gtheta\contamprior\right)^{2}\lambda\left(d\theta\right)\\
 & =\int\frac{p\left(x\vert\theta\right)}{\origprior}\left(\gtheta\contamprior\right)^{2}\lambda\left(d\theta\right).
\end{align*}

\end_inset

This could be infinite if 
\begin_inset Formula $\origprior$
\end_inset

 has lighter tails than 
\begin_inset Formula $p\left(x\vert\theta\right)\left(\gtheta\contamprior\right)^{2}$
\end_inset

.
 For example, the variance will be infinite if 
\begin_inset Formula $\origprior$
\end_inset

 is a normal distribution, 
\begin_inset Formula $\gtheta=\theta$
\end_inset

, and both 
\begin_inset Formula $\contamprior$
\end_inset

 and 
\begin_inset Formula $p\left(x\vert\theta\right)$
\end_inset

 have tail behavior in 
\begin_inset Formula $\theta$
\end_inset

 like a Student-t distribution.
 Although 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:vb_epsilon_sensitivity"

\end_inset

 may appear to have a similar problem, since we require
\begin_inset Formula 
\begin{align*}
\mbe_{\qthetapost}\left[\left(\frac{\contamprior}{\origprior}\right)^{2}\right] & =\int\frac{\qthetapost\left(\theta\right)\contamprior^{2}}{\origprior^{2}}\lambda\left(d\theta\right)
\end{align*}

\end_inset

to be finite, in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:vb_epsilon_sensitivity"

\end_inset

 we are able to use importance sampling to reduce the variance.
 Importance sampling is possible for VB because we have a closed parametric
 form for the VB posterior, whereas in MCMC we are constrained to use the
 samples from 
\begin_inset Formula $\pthetapost\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Section
Experiments
\begin_inset CommandInset label
LatexCommand label
name "sec:experiments"

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMLoadData, cache=glmm_cache
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMLoadData.R", echo=knitr_debug)	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMNumbers, cache=glmm_cache, results='asis'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMNumbers.R", print.eval=TRUE)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We demonstrate the techniques above on real data using a logistic regression
 with random effects, which is an example of a generalized linear mixed
 model (GLMM) 
\begin_inset CommandInset citation
LatexCommand citep
after "chapter 13"
key "agresti:2011:categorical"

\end_inset

.
 This data and model have several advantages as an illustration of our methods:
 the data set is large, the model contains a large number of imprecisely-estimat
ed latent variables (the unknown random effects), the model exhibits the
 sparsity of 
\begin_inset Formula $\klhess$
\end_inset

 that is typical in many VB applications, and the results admit an interesting
 comparison with maximum a posteriori estimates.
\end_layout

\begin_layout Standard
All the code necessary to clean the data, run the estimation procedures,
 and produce the results below can be found in the paper's git repository
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/rgiordan/CovariancesRobustnessVBPaper
\end_layout

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Data and model
\begin_inset CommandInset label
LatexCommand label
name "subsec:glmm_model"

\end_inset


\end_layout

\begin_layout Standard
We investigated a custom subsample of the 2014 Criteo Labs conversion logs
 dataset 
\begin_inset CommandInset citation
LatexCommand citep
key "criteo:2014:dataset"

\end_inset

, which contains an obfuscated sample of advertising data collected by Criteo
 over a period of two months.
 Each row of the dataset corresponds to an single user click on an online
 advertisement.
 For each click, the dataset records a binary outcome variable representing
 whether or not the user subsequently 
\begin_inset Quotes eld
\end_inset

converted
\begin_inset Quotes erd
\end_inset

 (i.e., performed a desired task, such as purchase a product or sign up for
 a mailing list).
 Each row contains two timestamps (which we ignore), eight numerical covariates,
 and nine factor-valued covariates.
 Of the eight numerical covariates, three contain 30% or more missing data,
 so we discarded them.
 We then applied a per-covariate normalizing transform to the distinct values
 of those remaining.
 Among the factor-valued covariates, we retained only the one with the largest
 number of unique values and discarded the others.
 These data-cleaning decisions were made for convenience.
 The goal of the present paper is to demonstrate our inference methods,
 not to draw conclusions about online advertising.
\end_layout

\begin_layout Standard
Although the meaning of the covariates has been obfuscated, for the purpose
 of discussion we will imagine that the single retained factor-valued covariate
 represents the identity of the advertiser, and the numeric covariates represent
 salient features of the user and/or the advertiser (e.g., how often the user
 has clicked or converted in the past, a machine learning rating for the
 advertisement quality, etc.).
 As such, it makes sense to model the probability of each row's binary outcome
 (whether or not the user converted) as a function of the five numeric covariate
s and the advertiser identity using a logistic GLMM.
 Specifically, we observe binary conversion outcomes, 
\begin_inset Formula $y_{it}$
\end_inset

 for click 
\begin_inset Formula $i$
\end_inset

 on advertiser 
\begin_inset Formula $t$
\end_inset

, with probabilities given by observed numerical explanatory variables,
 
\begin_inset Formula $x_{it}$
\end_inset

, each of which are vectors of length 
\begin_inset Formula $K_{x}=\glmmDimension$
\end_inset

.
 Additionally, the outcomes within a given value of 
\begin_inset Formula $t$
\end_inset

 are correlated through an unobserved random effect, 
\begin_inset Formula $u_{t}$
\end_inset

, which represents the 
\begin_inset Quotes eld
\end_inset

quality
\begin_inset Quotes erd
\end_inset

 of advertiser 
\begin_inset Formula $t$
\end_inset

, where the value of 
\begin_inset Formula $t$
\end_inset

 for each observation is given by the factor-valued covariate.
 The random effects 
\begin_inset Formula $u_{t}$
\end_inset

 are assumed to follow a normal distribution with unknown mean and variance.
 Formally,
\begin_inset Formula 
\begin{eqnarray*}
y_{it}\vert p_{it},\beta,u_{t},x_{it} & \sim & \textrm{Bernoulli}\left(p_{it}\right),\textrm{ for }t=1,...,T\textrm{ and }i=1,...,N\\
p_{it} & := & \frac{e^{\rho_{it}}}{1+e^{\rho_{it}}}\quad\textrm{where}\quad\rho_{it}:=x_{it}^{T}\beta+u_{t}\\
u_{t}\vert\mu,\tau & \sim & \mathcal{N}\left(\mu,\tau^{-1}\right).
\end{eqnarray*}

\end_inset

Consequently, the unknown parameters are 
\begin_inset Formula $\theta=\left(\beta^{\trans},\mu,\tau,u_{1},...,u_{T}\right)^{\trans}$
\end_inset

.
 We use the following priors:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mu\vert\mu_{0},\tau_{\mu} & \sim & \mathcal{N}\left(\mu_{0},\tau_{\mu}^{-1}\right)\\
\tau\vert\alpha_{\tau},\beta_{\tau} & \sim & \textrm{Gamma}\left(\alpha_{\tau},\beta_{\tau}\right)\\
\beta\vert\beta_{0},\tau_{\beta},\gamma_{\beta} & \sim & \mathcal{N}\left(\left(\begin{array}{c}
\beta_{0}\\
\vdots\\
\beta_{0}
\end{array}\right),\left(\begin{array}{ccc}
\tau_{\beta} & \gamma_{\beta} & \gamma_{\beta}\\
\gamma_{\beta} & \ddots & \gamma_{\beta}\\
\gamma_{\beta} & \gamma_{\beta} & \tau_{\beta}
\end{array}\right)^{-1}\right).
\end{eqnarray*}

\end_inset

Note that we initially take 
\begin_inset Formula $\gamma_{\beta}=0$
\end_inset

 so that the prior information matrix on 
\begin_inset Formula $\beta$
\end_inset

 is diagonal, though by retaining 
\begin_inset Formula $\gamma_{\beta}$
\end_inset

 as a hyperparameter we will be able to assess the sensitivity to the assumption
 of a diagonal prior in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_sensitivity"

\end_inset

.
 The remaining prior values are given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:glmm_details"

\end_inset

.
 It is reasonable to expect that a modeler would be interested both in the
 effect of the numerical covariates and in the quality of individual advertisers
 themselves, so we take the parameter of interest to be 
\begin_inset Formula $\gtheta=\left(\beta^{\trans},u_{1},...,u_{T}\right)^{\trans}$
\end_inset

.
 
\end_layout

\begin_layout Standard
To produce a dataset small enough to be amenable to MCMC but large and sparse
 enough to demonstrate our methods, we subsampled the data still further.
 We randomly chose 
\begin_inset Formula $\glmmNumGroups$
\end_inset

 distinct advertisers to analyze, and then subsampled each selected advertiser
 to contain no more than 20 rows each.
 The resulting dataset had 
\begin_inset Formula $N=\glmmNumObs$
\end_inset

 total rows.
 If we had more observations per advertiser, the 
\begin_inset Quotes eld
\end_inset

random effects
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $u_{t}$
\end_inset

 would have been estimated quite precisely, and the nonlinear nature of
 the problem would not have been important, obscuring the benefits of using
 VB versus, say, a maximum a posteriori (MAP) estimate.
 (We compare our results to a MAP estimator as well as a marginal maximum
 likelihood estimator in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_Means-and-variances"

\end_inset

 below.) In typical internet datasets a large amount of data comes from advertise
rs with few observations each, so our subsample is representative of practically
 interesting problems.
 
\end_layout

\begin_layout Subsection
Inference and timing
\begin_inset CommandInset label
LatexCommand label
name "subsec:glmm_inference"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMTimingTable, cache=glmm_cache, results='asis'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMTimingTable.R", print.eval=TRUE)
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Timing results
\begin_inset CommandInset label
LatexCommand label
name "tab:timing"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We estimated 
\begin_inset Formula $\epgtheta$
\end_inset

 using three techniques: MCMC, MFVB (including the sensitivity tools of
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:sensitivity_in_action"

\end_inset

), and the maximum a posteriori (MAP) estimate 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 13"
key "gelman:2014:bayesian"

\end_inset

.
 For MCMC, we used Stan 
\begin_inset CommandInset citation
LatexCommand citep
key "stan-manual:2015"

\end_inset

, and for both MFVB and MAP we used our own Python code using 
\family typewriter
numpy
\family default
, 
\family typewriter
scipy
\family default
, and 
\family typewriter
autograd
\family default
 
\begin_inset CommandInset citation
LatexCommand citep
key "scipy,maclaurin:2015:autograd"

\end_inset

.
 As described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_Means-and-variances"

\end_inset

, the MAP estimator did not estimate 
\begin_inset Formula $\epgtheta$
\end_inset

 very well, so we did not attempt to calculate standard deviations or sensitivit
y measures for the MAP estimator.
 The summary of the computation time for all these methods is shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:timing"

\end_inset

, with details below.
\end_layout

\begin_layout Standard
For the MCMC estimates, we used Stan to draw 
\begin_inset Formula $\glmmNumMCMCDraws$
\end_inset

 MCMC draws (not including warm-up), which took 
\begin_inset Formula $\glmmMCMCTimeMinutes$
\end_inset

 minutes.
 We estimated all the sensitivities in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_sensitivity"

\end_inset

 using the Monte Carlo version of the covariance in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:covariance_sensitivity"

\end_inset

.
 
\end_layout

\begin_layout Standard
For the variational approximation, we use the following mean field exponential
 family approximations:
\begin_inset Formula 
\begin{align*}
q\left(\beta_{k}\right) & =\normal\left(\beta_{k};\eta_{\beta_{k}}\right),\textrm{ for }k=1,...,K_{x}\\
q\left(u_{t}\right) & =\normal\left(u_{t};\eta_{u_{t}}\right),\textrm{ for }t=1,...,T\\
q\left(\tau\right) & =\textrm{Gamma}\left(\tau;\eta_{\tau}\right)\\
q\left(\mu\right) & =\normal\left(\mu;\eta_{\mu}\right)\\
q\left(\theta\right) & =q\left(\tau\right)q\left(\mu\right)\prod_{k=1}^{K_{x}}q\left(\beta_{k}\right)\prod_{t=1}^{T}q\left(u_{t}\right).
\end{align*}

\end_inset

With these choices, evaluating the variational objective requires the following
 intractable univariate variational expectation:
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{q}\left[\log\left(1-p_{it}\right)\right] & = & \mbe_{q}\left[\log\left(1-\frac{e^{\rho_{it}}}{1+e^{\rho_{it}}}\right)\right].
\end{eqnarray*}

\end_inset

We used the re-parameterization trick and four points of Gauss-Hermite quadratur
e to estimate this integral for each observation.
 See 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:glmm_details"

\end_inset

 for more details.
\end_layout

\begin_layout Standard
We optimized the variational objective using the conjugate gradient Newton's
 trust region method, 
\family typewriter
trust-ncg
\family default
, of 
\family typewriter
scipy.optimize
\family default
.
 One advantage of 
\family typewriter
trust-ncg
\family default
 is that it performs second-order optimization but requires only Hessian-vector
 products, which can be computed quickly by 
\family typewriter
autograd
\family default
 without constructing the full Hessian.
 The MFVB fit took 
\begin_inset Formula $\glmmVBTime$
\end_inset

 seconds, roughly 
\begin_inset Formula $\glmmSpeedup$
\end_inset

 times faster than MCMC with Stan.
\end_layout

\begin_layout Standard
With variational parameters for each random effect 
\begin_inset Formula $u_{t}$
\end_inset

, 
\begin_inset Formula $\klhess$
\end_inset

 is a 
\begin_inset Formula $\glmmHessDim\times\glmmHessDim$
\end_inset

 dimensional matrix.
 Consequently, evaluating 
\begin_inset Formula $\klhess$
\end_inset

 directly as a dense matrix using 
\family typewriter
autograd
\family default
 would have been prohibitively time-consuming.
 Fortunately, our model can be decomposed into global and local parameters,
 and the Hessian term 
\begin_inset Formula $\klhess$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 is extremely sparse.
 In the notation of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:lrvb_implementation"

\end_inset

, take 
\begin_inset Formula $\theta_{glob}=\left(\beta^{\trans},\mu,\tau\right)^{\trans}$
\end_inset

 , take 
\begin_inset Formula $\theta_{loc,t}=u_{t}$
\end_inset

, and stack the variational parameters as 
\begin_inset Formula $\eta=\left(\eta_{glob}^{\trans},\eta_{loc,1},...,\eta_{loc,T}\right)^{\trans}$
\end_inset

.
 The cross terms in 
\begin_inset Formula $\klhess$
\end_inset

 between the local variables vanish:
\begin_inset Formula 
\begin{align*}
\frac{\partial^{2}KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)}{\partial\eta_{loc,t_{1}}\partial\eta_{loc,t_{2}}} & =0\textrm{ for all }t_{1}\ne t_{2}.
\end{align*}

\end_inset

(Notice that the full likelihood in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "app:glmm_details"

\end_inset

 has no cross terms between 
\begin_inset Formula $u_{t_{1}}$
\end_inset

 and 
\begin_inset Formula $u_{t_{2}}$
\end_inset

 for 
\begin_inset Formula $t_{1}\ne t_{2}$
\end_inset

.) As the dimension 
\begin_inset Formula $T$
\end_inset

 of the data grows, so does the length of 
\begin_inset Formula $\eta$
\end_inset

.
 However, the dimension of 
\begin_inset Formula $\eta_{glob}$
\end_inset

 remains constant, and 
\begin_inset Formula $\klhess$
\end_inset

 remains easy to invert.
 We show an example of the sparsity pattern of the first few rows and columns
 of 
\begin_inset Formula $\klhess$
\end_inset

 in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMHessianSparsity}
\end_layout

\end_inset

 .
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename static_images/logit_sparsity.png
	width 2in
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Example sparsity of 
\begin_inset Formula $\klhess$
\end_inset

 for the logit GLMM model (black indicates non-zero entries)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:LogitGLMMHessianSparsity"

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Taking advantage of this sparsity pattern, we used 
\family typewriter
autograd
\family default
 to calculate the Hessian of the KL divergence one group at a time and assembled
 the results in a sparse matrix using the 
\family typewriter
scipy.sparse
\family default
 Python package.
 Even so, calculating the entire sparse Hessian took 
\begin_inset Formula $\glmmHessianTime$
\end_inset

 seconds, and solving the system 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 using 
\family typewriter
scipy.sparse.linalg.spsolve
\family default
 took an additional 
\begin_inset Formula $\glmmInverseTime$
\end_inset

 seconds.
 This means that the evaluation and inversion of 
\begin_inset Formula $\klhess$
\end_inset

 was several times more costly than optimizing the variational objective
 itself.
 (Of course, the whole procedure remains much faster than running MCMC with
 Stan.)
\end_layout

\begin_layout Standard
We note, however, that instead of the direct approach to calculating 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 one can use the conjugate gradient algorithm of 
\family typewriter
sp.sparse.linalg.cg
\family default
 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 5"
key "nocedalwright:1999:numerical"

\end_inset

 together with the fast Hessian-vector products of 
\family typewriter
autograd
\family default
 to query one column at a time of 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

.
 On a typical column of 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 in our experiment, calculating the conjugate gradient took only 
\begin_inset Formula $\glmmCGRowTime$
\end_inset

 seconds (corresponding to 
\begin_inset Formula $\glmmCGRowIters$
\end_inset

 Hessian-vector products in the conjugate gradient algorithm).
 Thus, for example, one could calculate the columns of 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 corresponding to the expectations of the global variables 
\begin_inset Formula $\beta$
\end_inset

 in only 
\begin_inset Formula $\glmmCGRowTime\times K_{x}=\glmmCGBetaTime$
\end_inset

 seconds, which is much less time than it would take to compute the entire
 
\begin_inset Formula $\klhess^{-1}\eggrad^{\trans}$
\end_inset

 for both 
\begin_inset Formula $\beta$
\end_inset

 and every random effect in 
\begin_inset Formula $u$
\end_inset

.
\end_layout

\begin_layout Standard
For an additional comparison with MFVB, we also calculated the MAP estimate:
\begin_inset Formula 
\begin{align*}
\theta_{map} & :=\argmax_{\theta}\pthetapost\left(\theta\right)=\argmax_{\theta}\log\pthetapost\left(\theta\right).
\end{align*}

\end_inset

We take 
\begin_inset Formula $\gtheta[\theta_{map}]$
\end_inset

 to be the MAP estimate of 
\begin_inset Formula $\epgtheta$
\end_inset

.
 The MAP estimator 
\begin_inset Formula $\theta_{map}$
\end_inset

, like the variational approximation 
\begin_inset Formula $\qthetapost\left(\theta\right)$
\end_inset

, is found by solving an optimization problem that does not require the
 normalizing constant of 
\begin_inset Formula $\pthetapost\left(\theta\right)$
\end_inset

.
 Indeed, the MAP estimator can be seen as equivalent to a MFVB approximation
 in which every parameter has a degenerate distribution that concentrates
 at a single point 
\begin_inset CommandInset citation
LatexCommand citep
key "neal:1998:variationalEM"

\end_inset

.
 Consequently, the MFVB approximation to posterior means would only be expected
 to improve on the MAP estimator in cases when there is both substantial
 uncertainty in some parameters and when this uncertainty, through nonlinear
 dependence between parameters, affects the values of posterior means.
 These circumstances obtain in the logistic GLMM model with sparse per-advertise
r data, since the random effects 
\begin_inset Formula $u_{t}$
\end_inset

 will be quite uncertain, and the other posterior means depend on them through
 the nonlinear logistic function.
 We calculated the MAP estimator using the same Python code used for the
 MFVB estimates.
\end_layout

\begin_layout Subsection
Posterior approximation results
\begin_inset CommandInset label
LatexCommand label
name "subsec:glmm_Means-and-variances"

\end_inset


\end_layout

\begin_layout Standard
In this section, we assess the accuracy of the MFVB and MAP estimators as
 approximations to 
\begin_inset Formula $\epgtheta$
\end_inset

 and 
\begin_inset Formula $\cov_{\pthetapost}\left(\gtheta\right)$
\end_inset

, taking the MCMC estimates as ground truth.
 Although, as discussed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_model"

\end_inset

, we are principally interested in the parameters 
\begin_inset Formula $\gtheta=\left(\beta^{\trans},u_{1},...,u_{T}\right)^{\trans}$
\end_inset

, we will report the results for all parameters for completeness.
 For readability, the tables and graphs show results for a random selection
 of the components of the random effects 
\begin_inset Formula $u$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Posterior means
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMMeanTable, cache=glmm_cache, results='asis'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMMeanTable.R", print.eval=TRUE)
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Results for the estimation of the posterior means
\begin_inset CommandInset label
LatexCommand label
name "tab:mean_results"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMMCMCComparisonMeans, cache=glmm_cache, fig.show='hold', fig.cap='Comparis
on of MCMC and MFVB means'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMMCMCComparisonMeans.R", echo=knitr_debug, print.eval=TRU
E)	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMMapComparisonMeans, cache=glmm_cache, fig.show='hold', fig.cap='Compariso
n of MCMC and MAP means'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMMapComparisonMeans.R", echo=knitr_debug, print.eval=TRUE
)	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We begin by comparing the posterior means in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:mean_results"

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMCMCComparisonMeans}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMapComparisonMeans}
\end_layout

\end_inset

.
 We first note that, despite the long running time for MCMC, the 
\begin_inset Formula $\beta_{1}$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

 parameters did not mix well in the MCMC sample, as is reflected in the
 MCMC standard error and effective number of draws columns of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:mean_results"

\end_inset

.
 The 
\begin_inset Formula $x_{it}$
\end_inset

 data corresponding to 
\begin_inset Formula $\beta_{1}$
\end_inset

 contained fewer distinct values than the other columns of 
\begin_inset Formula $x$
\end_inset

, which perhaps led to some co-linearity between 
\begin_inset Formula $\beta_{1}$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

 in the posterior.
 This could have caused both poor MCMC mixing and, perhaps, excessive prior
 sensitivity, as discussed below in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_sensitivity"

\end_inset

.
 Although we will report the results for both 
\begin_inset Formula $\beta_{1}$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

 without further comment, the reader should bear in mind that the MCMC 
\begin_inset Quotes eld
\end_inset

ground truth
\begin_inset Quotes erd
\end_inset

 for these two parameters is somewhat suspect.
\end_layout

\begin_layout Standard
The results in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:mean_results"

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMCMCComparisonMeans}
\end_layout

\end_inset

 show that MFVB does an excellent job of approximating the posterior means
 in this particular case, even for the random effects 
\begin_inset Formula $u$
\end_inset

 and the related parameters 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\tau$
\end_inset

.
 In contrast, the MAP estimator does reasonably well only for certain components
 of 
\begin_inset Formula $\beta$
\end_inset

 and does extremely poorly for the random effects parameters.
 As can be seen in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMapComparisonMeans}
\end_layout

\end_inset

, the MAP estimate dramatically overestimates the information 
\begin_inset Formula $\tau$
\end_inset

 of the random effect distribution (that is, it underestimates the variance).
 As a consequence, it estimates all the random effects to have essentially
 the same value, leading to mis-estimation of some location parameters,
 including both 
\begin_inset Formula $\mu$
\end_inset

 and some components of 
\begin_inset Formula $\beta$
\end_inset

.
 Because the MAP estimator performed so poorly at estimating the random
 effect means, we will not consider it any further.
\end_layout

\begin_layout Subsubsection
Posterior covariances
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMSdTable, cache=glmm_cache, results='asis'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMSdTable.R", print.eval=TRUE)
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Standard deviation results
\begin_inset CommandInset label
LatexCommand label
name "tab:sd_results"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMMCMCComparisonSds, cache=glmm_cache, fig.show='hold', fig.post='!h',
 fig.cap='Comparison of MCMC, MFVB, and LRVB standard deviations', out.width=im2h$
ow, out.height=im2h$oh, fig.width=im2h$fw, fig.height=im2h$fh
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMMCMCComparisonSds.R", echo=knitr_debug, print.eval=TRUE)
	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMMCMCComparisonCovariances, cache=glmm_cache, fig.show='hold', fig.post='!
h', fig.cap='Comparison of MCMC and LRVB off-diagonal covariances'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMMCMCComparisonCovariances.R", echo=knitr_debug,
 print.eval=TRUE)	
\end_layout

\end_inset

We now assess the accuracy of our estimates of 
\begin_inset Formula $\cov_{\pthetapost}\left(\gtheta\right)$
\end_inset

.
 The results for the diagonals (i.e., the marginal variances) are shown in
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:sd_results"

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMCMCComparisonSds}
\end_layout

\end_inset

.
 We refer to the diagonal of 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)$
\end_inset

 as the 
\begin_inset Quotes eld
\end_inset

uncorrected MFVB
\begin_inset Quotes erd
\end_inset

 estimate, and the diagonal of the linear response covariance estimate 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "def:lrvb_covariance"

\end_inset

 as the 
\begin_inset Quotes eld
\end_inset

LRVB
\begin_inset Quotes erd
\end_inset

 estimate.
 To make the scale more meaningful, we report standard deviations rather
 than variances.
 The uncorrected MFVB variance estimates 
\begin_inset Formula $\var_{\qthetapost}\left(\beta\right)$
\end_inset

 are particularly inaccurate, but the LRVB variances match the true posterior
 closely.
\end_layout

\begin_layout Standard
In 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMCMCComparisonCovariances}
\end_layout

\end_inset

, we compare the off-diagonal elements of 
\begin_inset Formula $\lrvbcov\left(\gtheta\right)$
\end_inset

 and 
\begin_inset Formula $\cov_{\pthetapost}\left(\gtheta\right)$
\end_inset

.
 These covariances are zero, by definition, in the uncorrected MFVB estimates
 
\begin_inset Formula $\cov_{\qthetapost}\left(\gtheta\right)$
\end_inset

.
 The left panel of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMMCMCComparisonCovariances}
\end_layout

\end_inset

 shows the estimated covariances between the global parameters and all other
 parameters, including the random effects, and the right panel shows only
 the covariances amongst the random effects.
 The LRVB covariances are quite accurate, particularly recalling that the
 MCMC draws of 
\begin_inset Formula $\mu$
\end_inset

 may be inaccurate due to poor mixing.
\end_layout

\begin_layout Subsection
Parametric sensitivity results
\begin_inset CommandInset label
LatexCommand label
name "subsec:glmm_sensitivity"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMPriorSensTable, cache=glmm_cache, results='asis'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMPriorSensTable.R", print.eval=TRUE)
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
MFVB normalized prior sensitivity results
\begin_inset CommandInset label
LatexCommand label
name "tab:prior_sens"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMParametricRobustness, cache=glmm_cache, fig.show='hold', fig.cap='Compari
son of MCMC and MFVB normalized parametric sensitivity results'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMParametricRobustness.R", echo=knitr_debug, print.eval=TR
UE)	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
LogitGLMMRefit, cache=glmm_cache, fig.show='hold', fig.cap='VB sensitivity
 as measured both by linear approximation and re-fitting'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

source("R_graphs/LogitGLMMRefitSensitivity.R", echo=knitr_debug, print.eval=TRUE)	
\end_layout

\end_inset

Finally, we compare the MFVB prior sensitivity measures of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Parametric-sensitivity"

\end_inset

 to the covariance-based MCMC sensitivity measures of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:local_sensitivity"

\end_inset

.
 Since sensitivity is of practical interest only when it is of comparable
 order as the posterior uncertainty, we report sensitivities normalized
 by the appropriate standard deviation.
 That is, we report 
\begin_inset Formula $\psenshat/\sqrt{\textrm{diag}\left(\hat{\cov}_{\pthetapost}\left(\gtheta\right)\right)}$
\end_inset

, and 
\begin_inset Formula $\qsens/\sqrt{\textrm{diag}\left(\lrvbcov\left(\gtheta\right)\right)}$
\end_inset

, etc., where 
\begin_inset Formula $\textrm{diag}\left(\cdot\right)$
\end_inset

 denotes the diagonal vector of a matrix, and the division is element-wise.
 Note that we use the sensitivity-based variance estimates 
\begin_inset Formula $\lrvbcov$
\end_inset

, not the uncorrected MFVB estimates 
\begin_inset Formula $\cov_{\qthetapost}$
\end_inset

, to normalize the variational sensitivities.
 We refer to a sensitivity divided by a standard deviation as a 
\begin_inset Quotes eld
\end_inset

normalized
\begin_inset Quotes erd
\end_inset

 sensitivity.
 
\end_layout

\begin_layout Standard
The comparison between the MCMC and MFVB sensitivity measures is shown in
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMParametricRobustness}
\end_layout

\end_inset

.
 The MFVB and MCMC sensitivities correspond very closely, though the MFVB
 means appear to be slightly more sensitive to the prior parameters than
 the MCMC means.
 This close correspondence should not be surprising.
 As shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:glmm_Means-and-variances"

\end_inset

, the MFVB and MCMC posterior means match quite closely.
 If we assume, reasonably, that they continue to match in a neighborhood
 of our original prior parameters, then 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:vb_accurate"

\end_inset

 will hold and we would expect 
\begin_inset Formula $\psenshat\approx\qsens$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:prior_sens"

\end_inset

 shows the detailed MFVB normalized sensitivity results.
 Each entry is the sensitivity of the VB mean of the row's parameter to
 the column's prior parameter.
 One can see that several parameters are quite sensitive to the information
 parameter prior 
\begin_inset Formula $\tau_{\mu}.$
\end_inset

 In particular, 
\begin_inset Formula $\mbe_{\pthetapost}\left[\mu\right]$
\end_inset

 and 
\begin_inset Formula $\mbe_{\pthetapost}\left[\beta_{1}\right]$
\end_inset

 are expected to change approximately 
\begin_inset Formula $-0.39$
\end_inset

 and 
\begin_inset Formula $-0.35$
\end_inset

 standard deviations, respectively, for every unit change in 
\begin_inset Formula $\tau_{\mu}$
\end_inset

.
 This size of change could be practically significant (assuming that such
 a change in 
\begin_inset Formula $\tau_{\mu}$
\end_inset

 is subjectively plausible).
 To investigate this sensitivity further, we re-fit the MFVB model at a
 range of values of the prior parameter 
\begin_inset Formula $\tau_{\mu}$
\end_inset

, assessing the accuracy of the linear approximation to the sensitivity.
 The results are shown in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMRefit}
\end_layout

\end_inset

.
 Even for very large changes in 
\begin_inset Formula $\tau_{\mu}$
\end_inset

-–-resulting in changes to 
\begin_inset Formula $\mbe_{\pthetapost}\left[\mu\right]$
\end_inset

 and 
\begin_inset Formula $\mbe_{\pthetapost}\left[\beta_{1}\right]$
\end_inset

 far in excess of two standard deviations-–-the linear approximation holds
 up reasonably well.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{LogitGLMMRefit}
\end_layout

\end_inset

 also shows a (randomly selected) random effect to be quite sensitive, though
 not to a practically important degree relative to its posterior standard
 deviation.
 The insensitivity of 
\begin_inset Formula $\mbe_{\pthetapost}\left[\beta_{2}\right]$
\end_inset

 is also confirmed.
 Of course, the accuracy of the linear approximation cannot be guaranteed
 to hold as well in general as it does in this particular case, and the
 quick and reliable evaluation of the linearity assumption without re-fitting
 the model remains interesting future work.
\end_layout

\begin_layout Standard
Because we started the MFVB optimization close to the new, perturbed optimum,
 each new MFVB fit at took only 
\begin_inset Formula $\glmmVBRefitTime$
\end_inset

 seconds on average.
 Re-estimating the MCMC posterior so many times would have been extremely
 time-consuming.
 (Note that importance sampling would be useless for prior parameter changes
 that moved the posterior so far from the original draws.) The considerable
 sensitivity of this model to a particular prior parameter, which is perhaps
 surprising on such a large dataset, illustrates the value of having fast,
 general tools for discovering and evaluating prior sensitivity.
 Our framework provides just such a set of tools.
\end_layout

\begin_layout Section
Conclusion
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusion"

\end_inset


\end_layout

\begin_layout Standard
By calculating the sensitivity of VB posterior means to model perturbations,
 we are able to provide two important practical tools for VB posterior approxima
tions: improved variance estimates and measures of prior robustness.
 When VB models are implemented in software that supports automatic differentiat
ion, our methods are fast, scalable, and require little additional coding
 beyond the VB objective itself.
 In our experiments, we were able to calculate accurate posterior means,
 covariances, and prior sensitivity measures orders of magnitude faster
 than MCMC.
\end_layout

\begin_layout Section
Acknowledgements
\end_layout

\begin_layout Standard
Ryan Giordano's research was supported by the National Energy Research Scientifi
c Computing Center, a DOE Office of Science User Facility supported by the
 Office of Science of the U.S.
 Department of Energy under Contract number DE-AC02- 05CH11231.
 Tamara Broderick's research was supported in part by a Google Faculty Research
 Award and the Office of Naval Research under contract/grant number N00014-17-1-
2072.
 This work was also supported in part by a MURI award, W911NF-17-1-0304,
 from the Army Research Office.
\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "variational_robustness"
options "plainnat"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
appendixpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Local sensitivity and covariances
\begin_inset CommandInset label
LatexCommand label
name "app:sens_and_cov"

\end_inset


\end_layout

\begin_layout Standard
In this section we prove 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

.
 It is a straightforward consequence of the Lebesgue dominated convergence
 theorem.
 Versions of this theorem have appeared many times before (e.g., 
\begin_inset CommandInset citation
LatexCommand citet
key "diaconis:1986:consistency,basu:1996:local,gustafson:1996:localposterior,perez:2006:mcmc"

\end_inset

 to name a few in the robustness literature).
\end_layout

\begin_layout Assumption
\begin_inset Formula $\covdens$
\end_inset

 is continuously differentiable with respect to 
\begin_inset Formula $t$
\end_inset

, and there exist 
\begin_inset Formula $\lambda$
\end_inset

-integrable 
\begin_inset Formula $f_{0}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $f_{1}\left(\theta\right)$
\end_inset

 such that 
\begin_inset Formula $\left|\exp\left(\covdens\right)g\left(\theta\right)\right|<f_{0}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\left|\exp\left(\covdens\right)\right|<f_{1}\left(\theta\right)$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "assu:exchange_order"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Assumption
The quantity 
\begin_inset Formula $\exp\left(\covdens\right)$
\end_inset

 can be normalized with respect to 
\begin_inset Formula $\lambda$
\end_inset

, i.e., 
\begin_inset Formula $0<\int\exp\left(\covdens\right)\lambda\left(d\theta\right)<\infty$
\end_inset

 .
\begin_inset CommandInset label
LatexCommand label
name "assu:bayes_ok"

\end_inset


\end_layout

\begin_layout Assumption
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
When these assumptions hold for all 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero, we can prove 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

.
\end_layout

\begin_layout Proof
Under 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

, we can exchange differentiation and integration in 
\begin_inset Formula $\int\exp\left(\covdens\right)g\left(\theta\right)\lambda\left(d\theta\right)$
\end_inset

 and 
\begin_inset Formula $\int\exp\left(\covdens\right)\lambda\left(d\theta\right)$
\end_inset

 by 
\begin_inset CommandInset citation
LatexCommand citet
after "Chapter 5-11, Theorem 18"
key "fleming:1965:functions"

\end_inset

, which ultimately depends on the Lebesgue dominated convergence theorem.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:bayes_ok"

\end_inset

, 
\begin_inset Formula $\mbe_{\covdens}\left[g\left(\theta\right)\right]$
\end_inset

 is well-defined in a neighborhood of 
\begin_inset Formula $t=0$
\end_inset

, and
\begin_inset Formula 
\begin{align*}
\frac{\partial\exp\left(\covdens\right)}{\partial t} & =\frac{\partial\covdens}{\partial t}\covdens\quad\lambda\textrm{-almost everywhere}.
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Armed with these facts, we can directly compute
\begin_inset Formula 
\begin{align*}
\left.\frac{\partial\mbe_{\covdens}\left[g\left(\theta\right)\right]}{\partial t^{\trans}}\right|_{t=0} & =\frac{\partial}{\partial t^{\trans}}\left.\frac{\int g\left(\theta\right)\exp\left(\covdens\right)\lambda\left(d\theta\right)}{\int\exp\left(\covdens\right)\lambda\left(d\theta\right)}\right|_{t=0}\\
 & =\left.\frac{\frac{\partial}{\partial t^{\trans}}\int g\left(\theta\right)\exp\left(\covdens\right)\lambda\left(d\theta\right)}{\int\exp\left(\covdens\right)\lambda\left(d\theta\right)}\right|_{t=0}-\mbe_{\covdens}\left[g\left(\theta\right)\right]\left.\frac{\frac{\partial}{\partial t^{\trans}}\int\exp\left(\covdens\right)\lambda\left(d\theta\right)}{\int\exp\left(\covdens\right)\lambda\left(d\theta\right)}\right|_{t=0}\\
 & =\left.\frac{\int g\left(\theta\right)\frac{\partial\covdens}{\partial t^{\trans}}\exp\left(\covdens\right)\lambda\left(d\theta\right)}{\int\exp\left(\covdens\right)\lambda\left(d\theta\right)}\right|_{t=0}-\mbe_{\covdens[][0]}\left[\gtheta\right]\left.\frac{\int\frac{\partial\covdens}{\partial t^{\trans}}\exp\left(\covdens\right)\lambda\left(d\theta\right)}{\int\exp\left(\covdens\right)\lambda\left(d\theta\right)}\right|_{t=0}\\
 & =\cov_{\covdens[][0]}\left(g\left(\theta\right),\left.\frac{\partial\covdens}{\partial t}\right|_{t=0}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Comparison with MCMC importance sampling
\begin_inset CommandInset label
LatexCommand label
name "app:mcmc_importance_sampling"

\end_inset


\end_layout

\begin_layout Standard
In this section, we show that using importance sampling with MCMC samples
 to calculate the local sensitivity 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:local_robustness"

\end_inset

 is precisely equivalent to using the same MCMC samples to estimate the
 covariance in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:covariance_sensitivity_general"

\end_inset

 directly.
 For this section, will suppose that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:bayes_ok"

\end_inset

 hold.
 Further suppose, without loss of generality, we have samples 
\begin_inset Formula $\theta_{i}$
\end_inset

 drawn iid from the normalized version of 
\begin_inset Formula $\exp\left(\covdens\right)$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\covdensnorm & :=\frac{\exp\left(\covdens\right)}{\int\exp\left(\covdens[\theta']\right)\lambda\left(d\theta'\right)}\\
\theta_{n} & \iid\covdensnorm[][0],\textrm{ for }n=1,...,N\\
\mbe_{\covdensnorm[][0]}\left[g\left(\theta\right)\right] & \approx\frac{1}{N}\sum_{n=1}^{N}g\left(\theta_{n}\right).
\end{align*}

\end_inset

Define the normalizing constant
\begin_inset Formula 
\begin{align*}
\tconst & :=\int\exp\left(\covdens[\theta']\right)\lambda\left(d\theta'\right).
\end{align*}

\end_inset

If we could calculate 
\begin_inset Formula $\tconst$
\end_inset

, we could use the following importance sampling estimate for 
\begin_inset Formula $\mbe_{\covdensnorm}\left[g\left(\theta\right)\right]$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\mbe_{\covdensnorm}\left[g\left(\theta\right)\right] & \approx\frac{1}{N}\sum_{n=1}^{N}w_{n}g\left(\theta_{n}\right)\\
w_{n} & :=\frac{\covdensnorm[\theta_{n}]}{\covdensnorm[\theta_{n}][0]}\\
 & =\exp\left(\covdens[\theta_{n}]-\covdens[\theta_{n}][0]+\log\tconst[0]-\log\tconst\right).
\end{align*}

\end_inset

Differentiating the weights,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial w_{n}}{\partial t} & =w_{n}\left(\frac{\partial\covdens[\theta_{n}]}{\partial t}-\frac{\partial\log\tconst}{\partial t}\right)\\
 & =w_{n}\left(\frac{\partial\covdens[\theta_{n}]}{\partial t}-\frac{1}{\tconst}\int\exp\left(\covdens\right)\frac{\partial\covdens}{\partial t}d\theta\right)\\
 & =w_{n}\left(\frac{\partial\covdens[\theta_{n}]}{\partial t}-\mbe_{\covdensnorm}\left[\frac{\partial\covdens}{\partial t}\right]\right).
\end{align*}

\end_inset

It follows that
\begin_inset Formula 
\begin{align*}
\left.\frac{\partial}{\partial t}\frac{1}{N}\sum_{n=1}^{N}w_{n}g\left(\theta_{n}\right)\right|_{t=0} & =\frac{1}{N}\sum_{n=1}^{N}\left(\left.\frac{\partial\covdens[\theta_{n}]}{\partial t}\right|_{t=0}-\mbe_{\covdensnorm[][0]}\left[\left.\frac{\partial\covdens}{\partial t}\right|_{t=0}\right]\right)g\left(\theta_{n}\right),
\end{align*}

\end_inset

which is precisely the MCMC sample estimate of the covariance given by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

.
\end_layout

\begin_layout Section
Our use of the terms 
\begin_inset Quotes eld
\end_inset

sensitivity
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

robustness
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "app:sens_and_robustness"

\end_inset


\end_layout

\begin_layout Standard
In this section we clarify our usage of the terms 
\begin_inset Quotes eld
\end_inset

robustness
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

sensitivity.
\begin_inset Quotes erd
\end_inset

 The quantity 
\begin_inset Formula $\psens^{\trans}\Delta\alpha$
\end_inset

 measures the 
\emph on
sensitivity
\emph default
 of 
\begin_inset Formula $\epgtheta$
\end_inset

 to perturbations in the direction 
\begin_inset Formula $\Delta\alpha$
\end_inset

.
 Intuitively, as sensitivity increases, robustness decreases, and, in this
 sense, sensitivity and robustness are opposites of one another.
 However, we emphasize that sensitivity is a clearly defined, measurable
 quantity and that robustness is a subjective judgment informed by sensitivity,
 but also by many other less objective considerations.
\end_layout

\begin_layout Standard
Suppose we have calculated the sensitivity to changes in the direction 
\begin_inset Formula $\Delta\alpha$
\end_inset

 from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:local_robustness"

\end_inset

 and found that it has a particular value.
 To determine whether our model is robust, we must additionally decide 
\end_layout

\begin_layout Enumerate
How large of a change in the prior, 
\begin_inset Formula $||\Delta\alpha||$
\end_inset

, is plausible, and 
\end_layout

\begin_layout Enumerate
How large of a change in 
\begin_inset Formula $\epgtheta$
\end_inset

 is important.
\end_layout

\begin_layout Standard
The set of plausible prior values necessarily remains a subjective decision
\begin_inset Foot
status open

\begin_layout Plain Layout
This decision can be cast in a formal decision theoretic framework based
 on a partial ordering of subjective beliefs.
 
\begin_inset CommandInset citation
LatexCommand citep
key "insua:2012:robustaxioms"

\end_inset


\end_layout

\end_inset

.
 Whether or not a particular change in 
\begin_inset Formula $\epgtheta$
\end_inset

 is important depends on the ultimate use of the posterior mean.
 For example, the posterior standard deviation can be a guide: if the prior
 sensitivity is swamped by the posterior uncertainty then it can be neglected
 when reporting our subjective uncertainty about 
\begin_inset Formula $g\left(\theta\right)$
\end_inset

, and the model is robust.
 Similarly, even if the prior sensitivity is much larger than the posterior
 standard deviation but small enough that it would not affect any actionable
 decision made on the basis of the value of 
\begin_inset Formula $\epgtheta$
\end_inset

, then the model is robust.
 Intermediate values remain a matter of judgment.
 A illustration of the relationship between sensitivity and robustness is
 shown in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
fig{robustness_vs_sensitivity}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename static_images/sensitivity_use.jpg
	lyxscale 15
	width 50text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
The relationship between robustness and sensitivity
\begin_inset CommandInset label
LatexCommand label
name "fig:robustness_vs_sensitivity"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, we note that if 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is small enough that 
\begin_inset Formula $\epgtheta$
\end_inset

 is roughly linear in 
\begin_inset Formula $\alpha$
\end_inset

 for 
\begin_inset Formula $\alpha\in\mathcal{A}$
\end_inset

, then calculating 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:local_robustness"

\end_inset

 for all 
\begin_inset Formula $\Delta\alpha\in\mathcal{A}-\alpha$
\end_inset

 and finding the worst case can be thought of as a first-order approximation
 to a global robustness estimate.
 Often, this linearity assumption is not plausible except for very small
 
\begin_inset Formula $\mathcal{A}$
\end_inset

, particularly for function-valued perturbations, as will be seen in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

.
 This is a weakness inherent to the local robustness approach.
 Nevertheless, even when the perturbations are valid only for a small 
\begin_inset Formula $\mathcal{A}$
\end_inset

, these easily-calculable measures can still provide valuable intuition
 about the potential modes of failure for a model.
\end_layout

\begin_layout Standard
When the prior has many parameters (i.e., 
\begin_inset Formula $\alpha$
\end_inset

 is high dimensional) many authors 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g."
key "basu:1996:local,gustafson:1996:localposterior,roos:2015:sensitivity"

\end_inset

 attempt to summarize the high-dimensional vector 
\begin_inset Formula $\psens$
\end_inset

 in a single easily reported number such as
\begin_inset Formula 
\begin{align*}
\psens^{sup} & :=\sup_{\alpha:\left\Vert \alpha\right\Vert \le1}\psens.
\end{align*}

\end_inset

Although this summary has obvious merits, in this work we do not attempt
 to summarize 
\begin_inset Formula $\psens$
\end_inset

 in this way for several reasons.
 First of all, the unit ball 
\begin_inset Formula $\left\Vert \alpha\right\Vert \le1$
\end_inset

 (as in 
\begin_inset CommandInset citation
LatexCommand citet
key "basu:1996:local"

\end_inset

) may not make sense as a subjective description of the range of plausible
 variability of 
\begin_inset Formula $\prior$
\end_inset

—why should the off-diagonal term of a Wishart prior plausibly vary as widely
 as the mean of some other parameter, when the two might not even have the
 same units? This problem might be easily remedied by choosing an appropriate
 scaling of the parameters and thereby making the unit ball an appropriate
 range for the problem at hand, but the right scaling will vary from problem
 to problem and necessarily be a somewhat subjective choice, so we refrain
 from taking a stand on this decision.
 
\end_layout

\begin_layout Section
Variational Bayes sensitivity derivations and assumptions
\begin_inset CommandInset label
LatexCommand label
name "app:lrvb"

\end_inset


\end_layout

\begin_layout Standard
Using the notation from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:vb_sensitivity"

\end_inset

, we will make the following assumptions:
\end_layout

\begin_layout Assumption
The posterior in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:p_tilting"

\end_inset

 is well-defined for 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero; i.e., there exists some 
\begin_inset Formula $\delta>0$
\end_inset

 such that 
\begin_inset CommandInset label
LatexCommand label
name "assu:tilt_exists"

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\int p\left(\theta\vert x\right)p\left(\theta\vert\alpha\right)\exp\left(f\left(\theta,t\right)\right)d\theta & < & \infty,\forall t\in\left\{ t:\left\Vert t\right\Vert _{2}<\delta\right\} .
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Assumption
There exists a local minimum, 
\begin_inset Formula $\etaopt$
\end_inset

, of 
\begin_inset Formula $KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:kl_divergence"

\end_inset

, such that 
\begin_inset Formula $\etaopt$
\end_inset

 is interior to 
\begin_inset Formula $\Omega_{\eta}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "assu:opt_interior"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Assumption
The KL divergence, 
\begin_inset Formula $KL\left(q\left(\theta;\eta\right)||\ptthetapost\left(\theta\right)\right)$
\end_inset

 is twice differentiable and strictly convex for 
\begin_inset Formula $\eta$
\end_inset

 in a neighborhood of the optimal 
\begin_inset Formula $\etaopt$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero.
 
\begin_inset CommandInset label
LatexCommand label
name "assu:kl_nice"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Assumption
The optimum 
\begin_inset Formula $\etatopt$
\end_inset

 of 
\begin_inset Formula $KL\left(q\left(\theta;\eta\right)||\ptthetapost\left(\theta\right)\right)$
\end_inset

 is a continuously differentiable function of 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of 
\begin_inset Formula $\etaopt=\etaopt\left(0\right)$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "assu:eta_t_smooth"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
We now prove 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

.
\end_layout

\begin_layout Proof
Under 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:tilt_exists"

\end_inset

, we can define a variational approximation to the tilted likelihood 
\begin_inset Formula $\ptthetapost\left(\theta\right)$
\end_inset

 that is a function of 
\begin_inset Formula $t$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\qtthetapost\left(\theta\right) & := & \textrm{argmin}_{q\in\mathcal{Q}}\left\{ KL\left(q\left(\theta;\eta\right)||\ptthetapost\left(\theta\right)\right)\right\} .
\end{eqnarray*}

\end_inset

 For notational convenience, we will define 
\begin_inset Formula 
\begin{eqnarray*}
KL\left(\eta,t\right) & := & KL\left(q\left(\theta;\eta\right)||\ptthetapost\left(\theta\right)\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
Since by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:opt_interior"

\end_inset

 
\begin_inset Formula $\etatopt$
\end_inset

 is both optimal and interior for all 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero, and by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

 
\begin_inset Formula $KL\left(\eta,t\right)$
\end_inset

 is smoothly differentiable in 
\begin_inset Formula $\eta$
\end_inset

, the first order conditions of the optimization problem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:kl_divergence"

\end_inset

 give:
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial KL\left(\eta,t\right)}{\partial\eta}\right|_{\eta=\etatopt} & = & 0.
\end{eqnarray*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

, we can take the total derivative of this vector-valued first order condition
 with respect to 
\begin_inset Formula $t$
\end_inset

, getting
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial^{2}KL\left(\eta,t\right)}{\partial\eta\partial\eta^{T}}\right|_{\eta=\etatopt}\frac{d\etatopt}{dt^{T}}+\left.\frac{\partial^{2}KL\left(\eta,t\right)}{\partial\eta\partial t^{T}}\right|_{\eta=\etatopt} & = & 0.
\end{eqnarray*}

\end_inset

The strict convexity of 
\begin_inset Formula $KL\left(\eta,t\right)$
\end_inset

 around 
\begin_inset Formula $\etaopt$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

 requires that 
\begin_inset Formula $\left.\frac{\partial^{2}KL\left(\eta,t\right)}{\partial\eta\partial\eta^{T}}\right|_{\eta=\etatopt}$
\end_inset

 be invertible, so by evaluating at 
\begin_inset Formula $t=0$
\end_inset

 and solving we find that 
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{d\etatopt}{dt^{T}}\right|_{t=0} & = & -\left.\left(\frac{\partial^{2}KL\left(\eta,t\right)}{\partial\eta\partial\eta^{T}}\right)^{-1}\frac{\partial^{2}KL\left(\eta,t\right)}{\partial\eta\partial t^{T}}\right|_{\eta=\etaopt,t=0}.
\end{eqnarray*}

\end_inset

Noting that 
\begin_inset Formula $\mbe_{\qtthetapost}\left[\gtheta\right]$
\end_inset

 is a function of 
\begin_inset Formula $\etatopt$
\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

 we have 
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{d\mbe_{\qtthetapost}\left[\gtheta\right]}{dt^{T}}\right|_{t=0} & = & \frac{\partial\mbeq\left[g\left(\theta\right)\right]}{\partial\eta}\left.\frac{d\etatopt}{dt^{T}}\right|_{\eta=\etaopt,t=0}.
\end{eqnarray*}

\end_inset

Finally, we observe that 
\begin_inset Formula 
\begin{eqnarray*}
KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta;t\right)\right) & = & \mbeq\left[\log q\left(\theta;\eta\right)-\log p\left(x\vert\theta\right)-\log p\left(\theta\vert\alpha\right)-f\left(\theta,t\right)\right]+\constant\Rightarrow\\
\left.\frac{\partial^{2}KL\left(\eta,t\right)}{\partial\eta\partial t^{T}}\right|_{t=0} & = & -\left.\frac{\partial^{2}\mbeq\left[f\left(\theta,t\right)\right]}{\partial\eta\partial t^{\trans}}\right|_{\eta=\etaopt,t=0}.
\end{eqnarray*}

\end_inset

Here, the term 
\begin_inset Formula $\constant$
\end_inset

 contains quantities that do not depend on 
\begin_inset Formula $\eta$
\end_inset

.
 Plugging in gives the desired result.
\end_layout

\begin_layout Section
Exactness of multivariate normal posterior means
\begin_inset CommandInset label
LatexCommand label
name "app:mvn_exact"

\end_inset


\end_layout

\begin_layout Standard
Here, we show that the MFVB estimate of the posterior means of a multivariate
 normal with known covariance is exact and that, as an immediate consequence,
 the linear response covariance recovers the exact posterior covariance,
 i.e., 
\begin_inset Formula $\lrvbcov\left(\theta\right)=\cov_{\pthetapost}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Suppose we are using MFVB to approximate a non-degenerate multivariate normal
 posterior, i.e.,
\begin_inset Formula 
\begin{align*}
\pthetapost\left(\theta\right) & =\mathcal{N}\left(\theta;\mu,\covmat\right)
\end{align*}

\end_inset

for full-rank 
\begin_inset Formula $\covmat$
\end_inset

.
 This posterior arises, for instance, given a multivariate normal likelihood
 
\begin_inset Formula $p\left(x\vert\mu\right)=\prod_{n=1:N}\mathcal{N}\left(x_{n}\vert\theta,\covmat_{x}\right)$
\end_inset

 with known covariance 
\begin_inset Formula $\covmat_{x}$
\end_inset

 and a conjugate multivariate normal prior on the unknown mean parameter
 
\begin_inset Formula $\theta$
\end_inset

.
 Additionally, even when the likelihood is non-normal or the prior is not
 conjugate, the posterior may be closely approximated by a multivariate
 normal distribution when a Bayesian central limit theorem can be applied
 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 8"
key "lecam:2012:asymptotics"

\end_inset

.
 We will consider an MFVB approximation to 
\begin_inset Formula $\pthetapost\left(\theta\right)$
\end_inset

.
 Specifically, let the elements of the vector 
\begin_inset Formula $\theta\in\mathbb{R}^{K}$
\end_inset

 be given by 
\begin_inset Formula $\theta_{k}$
\end_inset

, for 
\begin_inset Formula $k=1,...,K$
\end_inset

, and take the MFVB normal approximation with means 
\begin_inset Formula $m_{k}$
\end_inset

 and variances 
\begin_inset Formula $v_{k}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\mathcal{Q} & =\left\{ q\left(\theta\right):q\left(\theta\right)=\prod_{k=1}^{K}\mathcal{N}\left(\theta_{k};m_{k},v_{k}\right)\right\} .
\end{align*}

\end_inset

In the notation of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:q_mean_field_family"

\end_inset

, we have 
\begin_inset Formula $\eta_{k}=\left(m_{k},v_{k}\right)^{\trans}.$
\end_inset

 The optimal variational parameters are given by 
\begin_inset Formula $\eta_{k}^{*}=\left(m_{k}^{*},v_{k}^{*}\right)^{\trans}.$
\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:mvn_exact"

\end_inset

Let 
\begin_inset Formula $\pthetapost\left(\theta\right)=\mathcal{N}\left(\theta;\mu,\covmat\right)$
\end_inset

 for full-rank 
\begin_inset Formula $\covmat$
\end_inset

; let 
\begin_inset Formula $\mathcal{Q}=\left\{ q\left(\theta\right):q\left(\theta\right)=\prod_{k=1}^{K}\mathcal{N}\left(\theta_{k};m_{k},v_{k}\right)\right\} $
\end_inset

 be the mean field approximating family; and let the optimal parameter 
\begin_inset Formula $\etaopt=\left(m^{*},v^{*}\right)$
\end_inset

 solve 
\begin_inset Formula 
\begin{align*}
\etaopt & =\argmin_{\eta:q\left(\theta;\eta\right)\in\mathcal{Q}}KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right).
\end{align*}

\end_inset

Then 
\begin_inset Formula $m^{*}=\mu$
\end_inset

, i.e., the variational posterior means exactly recover the exact posterior
 means and 
\begin_inset Formula $\etaopt$
\end_inset

 is interior.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\textrm{diag}\left(v\right)$
\end_inset

 denote the 
\begin_inset Formula $K\times K$
\end_inset

 matrix with 
\begin_inset Formula $v$
\end_inset

 on the diagonal and zero elsewhere.
 Using the fact that the entropy of a univariate normal distribution with
 variance 
\begin_inset Formula $v$
\end_inset

 is 
\begin_inset Formula $\frac{1}{2}\log v$
\end_inset

 plus a constant, the variational objective 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:kl_divergence"

\end_inset

 is given by
\begin_inset Formula 
\begin{align*}
KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right) & =\mbe_{q\left(\theta;\eta\right)}\left[-\frac{1}{2}\left(\theta-\mu\right)^{\trans}\covmat^{-1}\left(\theta-\mu\right)\right]+\frac{1}{2}\sum_{k}\log v_{k}+C\\
 & =-\frac{1}{2}\textrm{trace}\left(\covmat^{-1}\mbe_{q\left(\theta;\eta\right)}\left[\theta\theta^{\trans}\right]\right)+\mu^{\trans}\covmat^{-1}\mbe_{q\left(\theta;\eta\right)}\left[\theta\right]+\frac{1}{2}\sum_{k}\log v_{k}+C\\
 & =-\frac{1}{2}\textrm{trace}\left(\covmat^{-1}\left(mm^{\trans}+\textrm{diag}\left(v\right)\right)\right)+\mu^{\trans}\covmat^{-1}m+\frac{1}{2}\sum_{k}\log v_{k}+C\\
 & =-\frac{1}{2}\textrm{trace}\left(\covmat^{-1}\textrm{diag}\left(v\right)\right)-\frac{1}{2}m^{\trans}\covmat^{-1}m+\mu^{\trans}\covmat^{-1}m+\frac{1}{2}\sum_{k}\log v_{k}+C.
\end{align*}

\end_inset

The first order condition for the optimal 
\begin_inset Formula $m^{*}$
\end_inset

 is then
\begin_inset Formula 
\begin{align*}
\left.\frac{\partial KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)}{\partial m}\right|_{m=m^{*},v=v^{*}} & =0\Rightarrow\\
-\covmat^{-1}m^{*}+\covmat^{-1}\mu & =0\Rightarrow\\
m^{*} & =\mu.
\end{align*}

\end_inset

The optimal variances follow similarly: 
\begin_inset Formula 
\begin{align*}
\left.\frac{\partial KL\left(q\left(\theta;\eta\right)||\pthetapost\left(\theta\right)\right)}{\partial v_{k}}\right|_{m=m^{*},v=v^{*}} & =0\Rightarrow\\
-\frac{1}{2}\left(\covmat^{-1}\right)_{kk}+\frac{1}{2}\frac{1}{v_{k}^{*}} & =0\Rightarrow\\
v_{k}^{*} & =\frac{1}{\left(\covmat^{-1}\right)_{kk}}.
\end{align*}

\end_inset

Clearly, both 
\begin_inset Formula $m^{*}$
\end_inset

 and 
\begin_inset Formula $v^{*}$
\end_inset

 are interior.
 The same result can be derived via the variational coordinate ascent updates
 (
\begin_inset CommandInset citation
LatexCommand citet
after "Section 10.1.2"
key "bishop:2006:pattern"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
after "Appendix B"
key "giordano:2015:lrvb"

\end_inset

).
\end_layout

\begin_layout Standard
Next, we show that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_exact"

\end_inset

 holds for all perturbations in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:p_tilting"

\end_inset

 of the form 
\begin_inset Formula $f\left(\theta,t\right)=t^{\trans}\theta$
\end_inset

 and that the assumptions necessary for application of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 are satisfied.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:mvn_perturbation_exact"

\end_inset

Under the conditions of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_exact"

\end_inset

, let 
\begin_inset Formula $\ptthetapost\left(\theta\right)$
\end_inset

 be defined from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:p_tilting"

\end_inset

 with 
\begin_inset Formula $f\left(\theta,t\right)=t^{\trans}\theta$
\end_inset

 and let 
\begin_inset Formula $\qtthetapost\left(\theta\right)$
\end_inset

 satisfy 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:perturbed_vb_approximation"

\end_inset

 with the approximating family given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_exact"

\end_inset

.
 Then 
\begin_inset Formula $\mbe_{\qtthetapost}\left[\theta\right]=\mbe_{\ptthetapost}\left[\theta\right]$
\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:tilt_exists"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:opt_interior"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

 are satisfied for all 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero.
 
\end_layout

\begin_layout Proof
By standard properties of the multivariate normal distribution, 
\begin_inset Formula $\ptthetapost\left(\theta\right)$
\end_inset

 is multivariate normal for any 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero when 
\begin_inset Formula $f\left(\theta,t\right)=t^{\trans}\theta$
\end_inset

.
 This property holds since 
\begin_inset Formula $\theta$
\end_inset

 is a sufficient statistic of the multivariate normal distribution, and
 the corresponding natural parameter of 
\begin_inset Formula $\pthetapost\left(\theta\right),$
\end_inset

 
\begin_inset Formula $\covmat^{-1}\mu$
\end_inset

, is interior when 
\begin_inset Formula $\covmat$
\end_inset

 is full-rank.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:tilt_exists"

\end_inset

 follows because the non-degenerate multivariate normal distribution is
 normalizable, and, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_exact"

\end_inset

, we have that 
\begin_inset Formula $\mbe_{\qtthetapost}\left[\theta\right]=\mbe_{\ptthetapost}\left[\theta\right]$
\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:opt_interior"

\end_inset

 holds.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:kl_nice"

\end_inset

 can be confirmed by directly inspecting the form of the 
\begin_inset Formula $\mbeq\left[\log\ptthetapost\left(\theta\right)\right]$
\end_inset

 term in the perturbed KL divergence:
\begin_inset Formula 
\begin{align*}
\mbeq\left[\log\ptthetapost\left(\theta\right)\right] & =-\frac{1}{2}\textrm{trace}\left(\covmat^{-1}\textrm{diag}\left(v\right)\right)-\frac{1}{2}m^{\trans}\covmat^{-1}m+\mu^{\trans}\covmat^{-1}m\\
 & \quad-\frac{1}{2}\sum_{k}\log v_{k}+t^{\trans}m+\constant.
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Here, 
\begin_inset Formula $\constant$
\end_inset

 contains quantities that do not depend on the variational distribution.
 Finally, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:eta_t_smooth"

\end_inset

 holds because, as shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_exact"

\end_inset

, 
\begin_inset Formula $\etaopt$
\end_inset

 is a smooth function of the natural exponential family parameters of 
\begin_inset Formula $\ptthetapost\left(\theta\right)$
\end_inset

, which are in turn smooth functions of 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
It now follows that the linear response variational covariance exactly reproduce
s the exact posterior covariance.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:mvn_cov_exact"

\end_inset

Under the conditions of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_exact"

\end_inset

, 
\begin_inset Formula $\lrvbcov\left(\theta\right)=\cov_{\pthetapost}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Proof
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_perturbation_exact"

\end_inset

 we have that, for all 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero, 
\begin_inset Formula $\mbe_{\qtthetapost}\left[\theta\right]=\mbe_{\ptthetapost}\left[\theta\right]$
\end_inset

.
 It follows that
\begin_inset Formula 
\begin{align*}
\left.\frac{d\mbe_{\qtthetapost}\left[\theta\right]}{dt}\right|_{t=0} & =\left.\frac{d\mbe_{\ptthetapost}\left[\theta\right]}{dt}\right|_{t=0}.
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Since, as argued in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_perturbation_exact"

\end_inset

, 
\begin_inset Formula $\ptthetapost\left(\theta\right)$
\end_inset

 is multivariate normal for 
\begin_inset Formula $t$
\end_inset

 in a neighborhood of zero, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:exchange_order"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "assu:bayes_ok"

\end_inset

 are satisfied, so we can apply 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sens_cov"

\end_inset

 to get
\begin_inset Formula 
\begin{align*}
\left.\frac{d\mbe_{\ptthetapost}\left[\theta\right]}{dt}\right|_{t=0} & =\cov_{\pthetapost}\left(\theta\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Finally, since by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mvn_perturbation_exact"

\end_inset

 the conditions for application of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lrvb_formula"

\end_inset

 are satisfied with 
\begin_inset Formula $g\left(\theta\right)=\theta$
\end_inset

, we have
\begin_inset Formula 
\begin{align*}
\left.\frac{d\mbe_{\qtthetapost}\left[\theta\right]}{dt}\right|_{t=0} & =\lrvbcov\left(\theta\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
It follows directly that 
\begin_inset Formula $\lrvbcov\left(\theta\right)=\cov_{\pthetapost}\left(\theta\right)$
\end_inset

 .
\end_layout

\begin_layout Section
LKJ Priors for Covariance Matrices in Mean Field Variational Inference
\begin_inset CommandInset label
LatexCommand label
name "app:lkj"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lkjdiagmat}{\mathbf{S}}
{\mathbf{S}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lkjcorrmat}{\mathbf{R}}
{\mathbf{R}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\lkjlocmat}{\mathbf{V}}
{\mathbf{V}}
\end_inset


\end_layout

\begin_layout Standard
In this section we briefly derive closed-form expressions for using an LKJ
 prior with a Wishart variational approximation.
\end_layout

\begin_layout Proposition
Let 
\begin_inset Formula $\covmat$
\end_inset

 be a 
\begin_inset Formula $K\times K$
\end_inset

 positive definite covariance matrix.
 Define the 
\begin_inset Formula $K\times K$
\end_inset

 matrix 
\begin_inset Formula $\lkjdiagmat$
\end_inset

 such that
\begin_inset Formula 
\begin{align*}
\lkjdiagmat_{ij} & =\begin{cases}
\sqrt{\covmat_{ij}} & \textrm{if }i=j\\
0 & \textrm{otherwise}.
\end{cases}
\end{align*}

\end_inset

Define the correlation matrix 
\begin_inset Formula $R$
\end_inset

 as
\begin_inset Formula 
\begin{align*}
\lkjcorrmat & =\lkjdiagmat^{-1}\covmat\lkjdiagmat^{-1}.
\end{align*}

\end_inset

Define the LKJ prior on 
\begin_inset Formula $\lkjcorrmat$
\end_inset

 with concentration parameter 
\begin_inset Formula $\xi$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "lewandowski:2009:lkj"

\end_inset

:
\begin_inset Formula 
\begin{align*}
p\left(\lkjcorrmat\vert\xi\right) & \propto\left|\lkjcorrmat\right|^{\xi-1}.
\end{align*}

\end_inset

Let 
\begin_inset Formula $q\left(\covmat\vert\lkjlocmat^{-1},\nu\right)$
\end_inset

 be an inverse Wishart distribution with matrix parameter 
\begin_inset Formula $\lkjlocmat^{-1}$
\end_inset

 and degrees of freedom 
\begin_inset Formula $\nu$
\end_inset

.
 Then
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{q}\left[\log\left|\lkjcorrmat\right|\right] & =\log\left|\lkjlocmat^{-1}\right|-\psi_{K}\left(\frac{\nu}{2}\right)-\sum_{k=1}^{K}\log\left(\left(\lkjlocmat^{-1}\right)_{kk}\right)-K\psi\left(\frac{\nu-K+1}{2}\right)+\constant\\
\mbe_{q}\left[\log p\left(\lkjcorrmat\vert\xi\right)\right] & =\left(\xi-1\right)\mathbb{E}_{q}\left[\log\left|\lkjcorrmat\right|\right],
\end{align*}

\end_inset

where 
\begin_inset Formula $\constant$
\end_inset

 does not depend on 
\begin_inset Formula $\lkjlocmat$
\end_inset

 or 
\begin_inset Formula $\nu$
\end_inset

.
 Here, 
\begin_inset Formula $\psi_{K}$
\end_inset

 is the multivariate digamma function.
\end_layout

\begin_layout Proof
First note that
\begin_inset Formula 
\begin{align*}
\log\left|\covmat\right| & =2\log\left|\lkjdiagmat\right|+\log\left|\lkjcorrmat\right|=\sum_{k=1}^{K}\log\lkjdiagmat_{k}^{2}+\log\left|\lkjcorrmat\right|=\sum_{k=1}^{K}\log\covmat_{kk}+\log\left|\lkjcorrmat\right|\Rightarrow\\
\log\left|\lkjcorrmat\right| & =\log\left|\covmat\right|-\sum_{k=1}^{K}\log\covmat_{kk}.
\end{align*}

\end_inset

By properties of the inverse Wishart distribution,
\begin_inset Formula 
\begin{align*}
E_{q}\left[\log\left|\covmat\right|\right] & =\log\left|\lkjlocmat^{-1}\right|-\psi_{K}\left(\frac{\nu}{2}\right)-K\log2,
\end{align*}

\end_inset

where 
\begin_inset Formula $\psi_{p}$
\end_inset

 is the multivariate digamma function.
 By the marginalization property of the inverse Wishart distribution,
\begin_inset Formula 
\begin{align*}
\covmat_{kk} & \sim\textrm{InverseWishart}\left(\left(\lkjlocmat^{-1}\right)_{kk},\nu-K+1\right)\Rightarrow\\
E_{q}\left[\log\covmat_{kk}\right] & =\log\left(\left(\lkjlocmat^{-1}\right)_{kk}\right)-\psi\left(\frac{\nu-K+1}{2}\right)-\log2.
\end{align*}

\end_inset

Plugging in gives the desired result.
\end_layout

\begin_layout Section
Logistic GLMM Model Details
\begin_inset CommandInset label
LatexCommand label
name "app:glmm_details"

\end_inset


\end_layout

\begin_layout Standard
In this section we include extra details about the model and analysis of
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:experiments"

\end_inset

.
 We will continue to use the notation defined therein.
 Denoting by 
\begin_inset Formula $\constant$
\end_inset

 constants that do not depend on the prior parameters, parameters, or data,
 the log likelihood is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{it}\vert u_{t},\beta\right) & = & y_{it}\log\left(\frac{p_{it}}{1-p_{it}}\right)+\log\left(1-p_{it}\right)\\
 & = & y_{it}\rho+\log\left(1-p_{it}\right)+\constant\\
\log p\left(u\vert\mu,\tau\right) & = & -\frac{1}{2}\tau\sum_{t=1}^{T}\left(u_{t}-\mu\right)^{2}-\frac{1}{2}T\log\tau\\
 & = & -\frac{1}{2}\tau\sum_{t=1}^{T}\left(u_{t}^{2}-\mu u_{t}+\mu^{2}\right)-\frac{1}{2}T\log\tau+\constant\\
\log p\left(\mu,\tau,\beta\right) & = & -\frac{1}{2}\sigma_{\mu}^{-2}\left(\mu^{2}+2\mu\mu_{0}\right)+\\
 &  & \left(1-\alpha_{\tau}\right)\tau+\beta_{\tau}\log\tau+\\
 &  & -\frac{1}{2}\left(\textrm{trace}\left(\covmat_{\beta}^{-1}\beta\beta^{T}\right)+2\textrm{trace}\left(\covmat_{\beta}^{-1}\beta_{0}\beta^{T}\right)\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The prior parameters were taken to be
\begin_inset Formula 
\begin{align*}
\mu_{0} & =\glmmMuLoc\\
\sigma_{\mu}^{-2} & =\glmmMuInfo\\
\beta_{0} & =\glmmBetaLoc\\
\sigma_{\beta}^{-2} & =\glmmBetaInfoDiag\\
\alpha_{\tau} & =\glmmTauAlpha\\
\beta_{\tau} & =\glmmTauBeta.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Under the variational approximation, 
\begin_inset Formula $\rho_{it}$
\end_inset

 is normally distributed given 
\begin_inset Formula $x_{it}$
\end_inset

, with 
\begin_inset Formula 
\begin{eqnarray*}
\rho_{it} & = & x_{it}^{T}\beta+u_{t}\\
\mbe_{q}\left[\rho_{it}\right] & = & x_{it}^{T}\mbe_{q}\left[\beta\right]+\mbe_{q}\left[u_{t}\right]\\
\textrm{Var}_{q}\left(\rho_{it}\right) & = & \mbe_{q}\left[\beta^{T}x_{it}x_{it}^{T}\beta\right]-\mbe_{q}\left[\beta\right]^{T}x_{it}x_{it}^{T}\mbe_{q}\left[\beta\right]+\textrm{Var}_{q}\left(u_{t}\right)\\
 & = & \mbe_{q}\left[\textrm{tr}\left(\beta^{T}x_{it}x_{it}^{T}\beta\right)\right]-\textrm{tr}\left(\mbe_{q}\left[\beta\right]^{T}x_{it}x_{it}^{T}\mbe_{q}\left[\beta\right]\right)+\textrm{Var}_{q}\left(u_{t}\right)\\
 & = & \textrm{tr}\left(x_{it}x_{it}^{T}\left(\mbe_{q}\left[\beta\beta^{T}\right]-\mbe_{q}\left[\beta\right]\mbe_{q}\left[\beta\right]^{T}\right)\right)+\textrm{Var}_{q}\left(u_{t}\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We can thus use 
\begin_inset Formula $n_{MC}=\glmmNumGHPoints$
\end_inset

 points of Gauss-Hermite quadrature to numerically estimate 
\begin_inset Formula $\mbe_{q}\left[\log\left(1-\frac{e^{\rho}}{1+e^{\rho}}\right)\right]$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\rho_{it,s} & :=\sqrt{\textrm{Var}_{q}\left(\rho_{it}\right)}z_{s}+\mbe_{q}\left[\rho_{it}\right]\\
\mbe_{q}\left[\log\left(1-\frac{e^{\rho_{it}}}{1+e^{\rho_{it}}}\right)\right] & \approx\frac{1}{n_{MC}}\sum_{s=1}^{n_{MC}}\log\left(1-\frac{e^{\rho_{it,s}}}{1+e^{\rho_{it,s}}}\right)
\end{align*}

\end_inset

We found that increasing the number of points used for the quadrature did
 not measurably change any of the results.
 The integration points and weights were calculated using the 
\family typewriter
\shape italic
numpy.polynomial.hermite
\family default
\shape default
 module in python 
\begin_inset CommandInset citation
LatexCommand citep
key "scipy"

\end_inset

.
\end_layout

\end_body
\end_document
